/home/wangdx_lab/cse11911612/.conda/envs/my_env1/lib/python3.9/site-packages/torch/nn/modules/conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return F.conv2d(input, weight, bias, self.stride,
original dir:  /home/wangdx_lab/cse11911612/SUSTech-Coded-Computation/src
Data is ready!
当前任务为 IMAGENET10
当前模型为 VGG16
K: 4
R: 2
N: 6
data_shape: (3, 224, 224)
num_classes: 10
conv_output_size: (512, 6, 6)
fc_input_size: 18432
模型总大小为：130.158MB
base_model_path: ./base_model/VGG16/IMAGENET10/model.pth
模型总大小为：130.158MB
Sequential(
  (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (1): ReLU()
  (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (3): ReLU()
)
Sequential(
  (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (1): ReLU()
  (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (3): ReLU()
)
Sequential(
  (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (1): ReLU()
  (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (3): ReLU()
  (4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (5): ReLU()
)
Sequential(
  (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (1): ReLU()
  (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (3): ReLU()
  (4): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (5): ReLU()
)
Sequential(
  (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (1): ReLU()
  (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (3): ReLU()
  (4): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1))
  (5): ReLU()
)
Model is ready!
tensor([[-0.4455, -0.0867,  0.9539, -1.9792, -1.6900, -1.2047,  1.3355, -0.6854,
         -1.5313,  3.9631]], device='cuda:0')
Initial x shape: torch.Size([1, 3, 224, 224])
After conv_segment1, z shape: torch.Size([1, 64, 224, 224])
After maxpool_segment1, z shape: torch.Size([1, 64, 112, 112])
After conv_segment1, z shape: torch.Size([1, 128, 112, 112])
After maxpool_segment1, z shape: torch.Size([1, 128, 56, 56])
After conv_segment1, z shape: torch.Size([1, 256, 56, 56])
After maxpool_segment1, z shape: torch.Size([1, 256, 28, 28])
After conv_segment1, z shape: torch.Size([1, 512, 28, 28])
After maxpool_segment1, z shape: torch.Size([1, 512, 14, 14])
After conv_segment1, z shape: torch.Size([1, 512, 12, 12])
After maxpool_segment1, z shape: torch.Size([1, 512, 6, 6])
tensor([[-0.4455, -0.0867,  0.9539, -1.9792, -1.6900, -1.2047,  1.3355, -0.6854,
         -1.5313,  3.9631]], device='cuda:0')
True
CASE: 0
shape_after_conv_segment: torch.Size([1, 64, 224, 224])
split_conv_output_shape: (64, 224, 56)
current_original_data_shape: (3, 224, 224)
split_data_range: [(3, 224, 0, 58), (3, 224, 54, 114), (3, 224, 110, 170), (3, 224, 166, 224)]
split_data_shapes: [(3, 224, 60), (3, 224, 62), (3, 224, 62), (3, 224, 60)]
choose the first one as the split_data_shape: (3, 224, 60)
CASE: 1
shape_after_conv_segment: torch.Size([1, 128, 112, 112])
split_conv_output_shape: (128, 112, 28)
current_original_data_shape: torch.Size([64, 112, 112])
split_data_range: [(64, 112, 0, 30), (64, 112, 26, 58), (64, 112, 54, 86), (64, 112, 82, 112)]
split_data_shapes: [(64, 112, 32), (64, 112, 34), (64, 112, 34), (64, 112, 32)]
choose the first one as the split_data_shape: (64, 112, 32)
CASE: 2
shape_after_conv_segment: torch.Size([1, 256, 56, 56])
split_conv_output_shape: (256, 56, 14)
current_original_data_shape: torch.Size([128, 56, 56])
split_data_range: [(128, 56, 0, 17), (128, 56, 11, 31), (128, 56, 25, 45), (128, 56, 39, 56)]
split_data_shapes: [(128, 56, 20), (128, 56, 23), (128, 56, 23), (128, 56, 20)]
choose the first one as the split_data_shape: (128, 56, 20)
CASE: 3
shape_after_conv_segment: torch.Size([1, 512, 28, 28])
split_conv_output_shape: (512, 28, 7)
current_original_data_shape: torch.Size([256, 28, 28])
split_data_range: [(256, 28, 0, 10), (256, 28, 4, 17), (256, 28, 11, 24), (256, 28, 18, 28)]
split_data_shapes: [(256, 28, 13), (256, 28, 16), (256, 28, 16), (256, 28, 13)]
choose the first one as the split_data_shape: (256, 28, 13)
CASE: 4
shape_after_conv_segment: torch.Size([1, 512, 12, 12])
split_conv_output_shape: (512, 12, 3)
current_original_data_shape: torch.Size([512, 14, 14])
split_data_range: [(512, 14, 0, 8), (512, 14, 0, 11), (512, 14, 3, 14), (512, 14, 6, 14)]
split_data_shapes: [(512, 14, 11), (512, 14, 14), (512, 14, 14), (512, 14, 11)]
choose the first one as the split_data_shape: (512, 14, 11)
CASE: 0
K : 4 R : 2 split_data_shape: (3, 224, 60)
N : 6 K : 4 split_conv_output_shape: (64, 224, 56)
模型总大小为：1.135MB
模型总大小为：4.175MB
CASE: 1
K : 4 R : 2 split_data_shape: (64, 112, 32)
N : 6 K : 4 split_conv_output_shape: (128, 112, 28)
模型总大小为：1.843MB
模型总大小为：5.875MB
CASE: 2
K : 4 R : 2 split_data_shape: (128, 56, 20)
N : 6 K : 4 split_conv_output_shape: (256, 56, 14)
模型总大小为：2.586MB
模型总大小为：9.275MB
CASE: 3
K : 4 R : 2 split_data_shape: (256, 28, 13)
N : 6 K : 4 split_conv_output_shape: (512, 28, 7)
模型总大小为：4.071MB
模型总大小为：16.076MB
CASE: 4
K : 4 R : 2 split_data_shape: (512, 14, 11)
N : 6 K : 4 split_conv_output_shape: (512, 12, 3)
模型总大小为：7.042MB
模型总大小为：16.076MB
epoch_num: 3
Train dataset: 13000
image size:  torch.Size([3, 224, 224])
save_dir: ./save/IMAGENET10/VGG16/2024_06_22/K4-R2-conv-division-E-Dcoder-Num0/
encoder_path: ./save/IMAGENET10/VGG16/2024_06_22/K4-R2-conv-division-E-Dcoder-Num0/encoder-task_IMAGENET10-basemodel_VGG16-K4-R2.pth
decoder_path: ./save/IMAGENET10/VGG16/2024_06_22/K4-R2-conv-division-E-Dcoder-Num0/decoder-task_IMAGENET10-basemodel_VGG16-K4-R2.pth
save_dir: ./save/IMAGENET10/VGG16/2024_06_22/K4-R2-conv-division-E-Dcoder-Num1/
encoder_path: ./save/IMAGENET10/VGG16/2024_06_22/K4-R2-conv-division-E-Dcoder-Num1/encoder-task_IMAGENET10-basemodel_VGG16-K4-R2.pth
decoder_path: ./save/IMAGENET10/VGG16/2024_06_22/K4-R2-conv-division-E-Dcoder-Num1/decoder-task_IMAGENET10-basemodel_VGG16-K4-R2.pth
original dir:  /home/wangdx_lab/cse11911612/SUSTech-Coded-Computation/src
encoder_path: ./save/IMAGENET10/VGG16/2024_06_22/K4-R2-conv-division-E-Dcoder-Num0/encoder-task_IMAGENET10-basemodel_VGG16-K4-R2.pth
decoder_path: ./save/IMAGENET10/VGG16/2024_06_22/K4-R2-conv-division-E-Dcoder-Num0/decoder-task_IMAGENET10-basemodel_VGG16-K4-R2.pth
encoder_path: ./save/IMAGENET10/VGG16/2024_06_22/K4-R2-conv-division-E-Dcoder-Num1/encoder-task_IMAGENET10-basemodel_VGG16-K4-R2.pth
decoder_path: ./save/IMAGENET10/VGG16/2024_06_22/K4-R2-conv-division-E-Dcoder-Num1/decoder-task_IMAGENET10-basemodel_VGG16-K4-R2.pth
loss_num: 0
样本总数: 13000
原始模型(conv+fc) -> 预测正确数: 10938, 预测准确率: 84.14%
使用Encoder和Decoder -> 预测正确数: 9757, 预测准确率: 75.05%
loss_num: 1
样本总数: 13000
原始模型(conv+fc) -> 预测正确数: 10938, 预测准确率: 84.14%
使用Encoder和Decoder -> 预测正确数: 8837, 预测准确率: 67.98%
loss_num: 2
样本总数: 13000
原始模型(conv+fc) -> 预测正确数: 10938, 预测准确率: 84.14%
使用Encoder和Decoder -> 预测正确数: 7032, 预测准确率: 54.09%
loss_num: 3
样本总数: 13000
原始模型(conv+fc) -> 预测正确数: 10938, 预测准确率: 84.14%
使用Encoder和Decoder -> 预测正确数: 5371, 预测准确率: 48.17%
loss_num: 0
样本总数: 13000
原始模型(conv+fc) -> 预测正确数: 11150, 预测准确率: 85.77%
使用Encoder和Decoder -> 预测正确数: 9892, 预测准确率: 76.09%
loss_num: 1
样本总数: 13000
原始模型(conv+fc) -> 预测正确数: 11150, 预测准确率: 85.77%
使用Encoder和Decoder -> 预测正确数: 8849, 预测准确率: 68.07%
loss_num: 2
样本总数: 13000
原始模型(conv+fc) -> 预测正确数: 11150, 预测准确率: 85.77%
使用Encoder和Decoder -> 预测正确数: 7337, 预测准确率: 56.44%
loss_num: 3
样本总数: 13000
原始模型(conv+fc) -> 预测正确数: 11150, 预测准确率: 85.77%
使用Encoder和Decoder -> 预测正确数: 6027, 预测准确率: 46.36%
