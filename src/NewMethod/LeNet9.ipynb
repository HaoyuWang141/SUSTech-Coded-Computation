{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "original dir:  f:\\MyCourse(5 delayed 1)\\erasure code\\SUSTech-Coded-Computation\\src\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "\n",
    "print(\"original dir: \", os.getcwd())\n",
    "\n",
    "if os.getcwd().endswith(\"NewMethod\"):\n",
    "    new_path = \"../\"\n",
    "    os.chdir(new_path)\n",
    "    print(\"changed dir: \", os.getcwd())\n",
    "    \n",
    "import torch\n",
    "import torch.nn\n",
    "from torch import optim\n",
    "from torch.nn import CrossEntropyLoss\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "TASK_CONFIG = {\n",
    "    \"TASK\": \"CIFAR10\",\n",
    "    \"DATE\": datetime.datetime.now().strftime(\"%Y_%m_%d\"),\n",
    "    \"MODEL\": \"LeNet9\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "读取数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前任务为 CIFAR10\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Data is ready!\n"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# 设置数据转换\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))]\n",
    ")\n",
    "\n",
    "# 设置数据集（训练集与测试集合）\n",
    "\n",
    "\"\"\"\n",
    "MNIST:\n",
    "image: (1, 28, 28), label: (0-9)\n",
    "\n",
    "FashionMNIST:\n",
    "image: (1, 28, 28), label: (0-9)\n",
    "\n",
    "CIFAR10:\n",
    "image: (3, 32, 32), label: (0-9)\n",
    "\"\"\"\n",
    "\n",
    "print(f\"当前任务为 {TASK_CONFIG['TASK']}\")\n",
    "train_dataset = datasets.CIFAR10(\n",
    "    root=\"./data\", train=True, download=True, transform=transform,\n",
    ")\n",
    "test_dataset = datasets.CIFAR10(\n",
    "    root=\"./data\", train=False, download=True, transform=transform,\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "print(\"Data is ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "设置部分参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K: 4\n",
      "R: 2\n",
      "N: 6\n",
      "data_shape: (3, 32, 32)\n",
      "num_classes: 10\n"
     ]
    }
   ],
   "source": [
    "K = 4\n",
    "R = 2\n",
    "N = K + R\n",
    "original_data_shape = tuple(train_dataset[0][0].shape)\n",
    "num_classes = 10\n",
    "print(f\"K: {K}\")\n",
    "print(f\"R: {R}\")\n",
    "print(f\"N: {N}\")\n",
    "print(f\"data_shape: {original_data_shape}\")\n",
    "print(f\"num_classes: {num_classes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义 base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fc_input_size: 4096\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from base_model.MyModel1 import MyModel1\n",
    "from base_model.LeNet5 import LeNet5\n",
    "from base_model.LeNet9 import LeNet9\n",
    "\n",
    "# 引入 base model, 该model将在后续全部过程中使用\n",
    "# ResNet\n",
    "assert TASK_CONFIG[\"MODEL\"] == \"LeNet9\"\n",
    "model = LeNet9(input_dim=original_data_shape, num_classes=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_model_path: ./base_model/LeNet9/CIFAR10/model.pth\n",
      "Model is ready!\n"
     ]
    }
   ],
   "source": [
    "# 读取模型\n",
    "base_model_path = (\n",
    "    f\"./base_model/{TASK_CONFIG['MODEL']}/{TASK_CONFIG['TASK']}/model.pth\"\n",
    ")\n",
    "print(f\"base_model_path: {base_model_path}\")\n",
    "\n",
    "model.load_state_dict(torch.load(base_model_path, map_location=device))\n",
    "conv_segment = model.get_conv_segment()\n",
    "fc_segment = model.get_fc_segment()\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print(\"Model is ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "验证 base model 准确率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集-> 总量: 50000, 正确数量: 43124, 准确率: 86.248%\n",
      "测试集-> 总量: 10000, 正确数量: 7432, 准确率: 74.32%\n"
     ]
    }
   ],
   "source": [
    "# 测试循环\n",
    "model.eval()  # 设置模型为评估模式\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():  # 在评估过程中不计算梯度\n",
    "    for data, target in train_loader:\n",
    "        # 将数据移动到设备上\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        output = model(data)\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        total += target.size(0)\n",
    "        correct += (predicted == target).sum().item()\n",
    "\n",
    "print(f\"训练集-> 总量: {total}, 正确数量: {correct}, 准确率: {100 * correct / total}%\")\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():  # 在评估过程中不计算梯度\n",
    "    for data, target in test_loader:\n",
    "        # 将数据移动到设备上\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        output = model(data)\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        total += target.size(0)\n",
    "        correct += (predicted == target).sum().item()\n",
    "\n",
    "print(f\"测试集-> 总量: {total}, 正确数量: {correct}, 准确率: {100 * correct / total}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "验证 model 和 conv_segment, fc_segment 的输出是否一致"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.9263,  1.1661, -2.5750, -3.4881, -3.6475, -5.0416, -0.3305, -5.9655,\n",
      "          3.1479, -2.0112]], device='cuda:0')\n",
      "tensor([[ 0.9263,  1.1661, -2.5750, -3.4881, -3.6475, -5.0416, -0.3305, -5.9655,\n",
      "          3.1479, -2.0112]], device='cuda:0')\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(1, *original_data_shape).to(device)\n",
    "model.to(device)\n",
    "y = model(x)\n",
    "print(y.data)\n",
    "\n",
    "z = conv_segment(x)\n",
    "z = z.flatten(1)\n",
    "z = fc_segment(z)\n",
    "print(z.data)\n",
    "print(torch.allclose(y, z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "设置另一部分参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv_output_shape: (256, 4, 4)\n",
      "split_conv_output_shape: (256, 4, 1)\n",
      "split_data_range: [(3, 32, 0, 23), (3, 32, 4, 27), (3, 32, 8, 31), (3, 32, 12, 32)]\n",
      "split_conv_output_data_shape from split_data_shape: [(1, 256, 4, 1), (1, 256, 4, 1), (1, 256, 4, 1), (1, 256, 4, 1)]\n",
      "split_data_shapes: [(3, 32, 23), (3, 32, 23), (3, 32, 23), (3, 32, 20)]\n",
      "choose the first one as the split_data_shape: (3, 32, 23)\n"
     ]
    }
   ],
   "source": [
    "from util.util import cal_input_shape\n",
    "\n",
    "\n",
    "conv_output_shape = model.calculate_conv_output(input_dim=original_data_shape)\n",
    "print(f\"conv_output_shape: {conv_output_shape}\")\n",
    "assert conv_output_shape[2] % K == 0\n",
    "\n",
    "split_conv_output_shape = (\n",
    "    conv_output_shape[0],\n",
    "    conv_output_shape[1],\n",
    "    conv_output_shape[2] // K,\n",
    ")\n",
    "print(f\"split_conv_output_shape: {split_conv_output_shape}\")\n",
    "\n",
    "conv_segment.to('cpu')\n",
    "conv_segment.train()\n",
    "split_data_range = cal_input_shape(\n",
    "    model=conv_segment,\n",
    "    original_input_shape=original_data_shape,\n",
    "    original_output_shape=conv_output_shape,\n",
    "    split_num=K,\n",
    ")\n",
    "print(f\"split_data_range: {split_data_range}\")\n",
    "\n",
    "# print(conv_segment)\n",
    "print(\n",
    "    f\"split_conv_output_data_shape from split_data_shape: {[tuple(conv_segment(torch.randn(1, _[0], _[1], _[3] - _[2])).shape) for _ in split_data_range]}\"\n",
    ")\n",
    "\n",
    "split_data_shapes = [\n",
    "    (\n",
    "        _[0],\n",
    "        _[1],\n",
    "        _[3] - _[2],\n",
    "    )\n",
    "    for _ in split_data_range\n",
    "]\n",
    "print(f\"split_data_shapes: {split_data_shapes}\")\n",
    "\n",
    "split_data_shape = split_data_shapes[0]\n",
    "print(f\"choose the first one as the split_data_shape: {split_data_shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "验证分割后的输入，能够恰好恢复出原始输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y.shape: torch.Size([1, 256, 4, 4])\n",
      "y_split.shape: [(1, 256, 4, 1), (1, 256, 4, 1), (1, 256, 4, 1), (1, 256, 4, 1)]\n",
      "y_hat.shape: torch.Size([1, 256, 4, 4])\n",
      "y和y_hat是否相等: True\n",
      "y和y_hat是否相等: True\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(1, *original_data_shape).to(device)\n",
    "conv_segment.to(device)\n",
    "y = conv_segment(x)\n",
    "print(f\"y.shape: {y.shape}\")\n",
    "\n",
    "x_split = [x[:, :, :, _[2]:_[3]] for _ in split_data_range]\n",
    "y_split = [conv_segment(_x) for _x in x_split]\n",
    "print(f\"y_split.shape: {[tuple(_y.shape) for _y in y_split]}\")\n",
    "\n",
    "y_hat = torch.cat(y_split, dim=3)\n",
    "print(f\"y_hat.shape: {y_hat.shape}\")\n",
    "\n",
    "# |A-B| <= atol + rtol * |B|\n",
    "print(f\"y和y_hat是否相等: {torch.allclose(y_hat, y, rtol=1e-08, atol=1e-05)}\")\n",
    "\n",
    "diff = torch.abs(y_hat - y)\n",
    "epsilon = 0.0001\n",
    "print(f\"y和y_hat是否相等: {torch.all(diff <= epsilon)}\")\n",
    "# print(torch.allclose(y_split[0], y[:, :, :, 0:5]))\n",
    "# print(torch.allclose(y_split[1], y[:, :, :, 5:10]))\n",
    "# print(torch.allclose(y_split[2], y[:, :, :, 10:15]))\n",
    "# print(torch.allclose(y_split[3], y[:, :, :, 15:20]))\n",
    "\n",
    "# print(y[0][0][0] == y_hat[0][0][0])\n",
    "# print(y[0][0][0])\n",
    "# print(y_hat[0][0][0])\n",
    "# y = x\n",
    "# y_split = x_split\n",
    "# for layer in conv_segment:\n",
    "#     print(layer)\n",
    "#     y = layer(y)\n",
    "#     y_split = [layer(_y) for _y in y_split]\n",
    "#     print(f\"y.shape: {y.shape}\")\n",
    "#     print(f\"y_split.shape: {[tuple(_.shape) for _ in y_split]}\")\n",
    "#     print(y[0][0][0])\n",
    "#     print(y_split[0][0][0][0])\n",
    "#     print(y_split[1][0][0][0])\n",
    "#     print(y_split[2][0][0][0])\n",
    "#     print(y_split[3][0][0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义 Encoder Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split_data_shape: (3, 32, 23)\n",
      "split_conv_output_shape: (256, 4, 1)\n"
     ]
    }
   ],
   "source": [
    "from encoder.mlp_encoder import MLPEncoder\n",
    "from encoder.conv_encoder import CatChannelConvEncoder, CatBatchSizeConvEncoder\n",
    "from decoder.mlp_decoder import MLPDecoder\n",
    "from decoder.conv_decoder import CatChannelConvDecoder, CatBatchSizeConvDecoder\n",
    "\n",
    "print(f\"split_data_shape: {split_data_shape}\")\n",
    "print(f\"split_conv_output_shape: {split_conv_output_shape}\")\n",
    "\n",
    "encoder = MLPEncoder(num_in=K, num_out=R, in_dim=split_data_shape)\n",
    "decoder = MLPDecoder(num_in=N, num_out=K, in_dim=split_conv_output_shape)\n",
    "\n",
    "# encoder = CatChannelConvEncoder(num_in=K, num_out=R, in_dim=split_data_shape)\n",
    "# decoder = CatChannelConvDecoder(\n",
    "#     num_in=N, num_out=K, in_dim=split_conv_output_shape\n",
    "# )\n",
    "\n",
    "# encoder = CatBatchSizeConvEncoder(num_in=K, num_out=R, in_dim=split_data_shape)\n",
    "# decoder = CatBatchSizeConvDecoder(num_in=N, num_out=K, in_dim=split_conv_output_data_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(torch.cuda.memory_summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型总大小为：446.394MB\n",
      "模型总大小为：224.047MB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def getModelSize(model):\n",
    "    param_size = 0\n",
    "    param_sum = 0\n",
    "    for param in model.parameters():\n",
    "        param_size += param.nelement() * param.element_size()\n",
    "        param_sum += param.nelement()\n",
    "    buffer_size = 0\n",
    "    buffer_sum = 0\n",
    "    for buffer in model.buffers():\n",
    "        buffer_size += buffer.nelement() * buffer.element_size()\n",
    "        buffer_sum += buffer.nelement()\n",
    "    all_size = (param_size + buffer_size) / 1024 / 1024\n",
    "    print(\"模型总大小为：{:.3f}MB\".format(all_size))\n",
    "    return (param_size, param_sum, buffer_size, buffer_sum, all_size)\n",
    "\n",
    "getModelSize(encoder)\n",
    "getModelSize(decoder)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练 Encoder Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch_num: 10\n",
      "Train dataset: 50000\n",
      "image size:  torch.Size([3, 32, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|████████████████████| 391/391 [04:33<00:00,  1.43it/s, loss=0.0114]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Accuracy: 86.248%, Train Accuracy: 72.174%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|████████████████████| 391/391 [04:32<00:00,  1.43it/s, loss=0.00697]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Accuracy: 86.248%, Train Accuracy: 82.212%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|████████████████████| 391/391 [02:58<00:00,  2.19it/s, loss=0.00564]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Accuracy: 86.248%, Train Accuracy: 83.524%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|████████████████████| 391/391 [02:58<00:00,  2.19it/s, loss=0.0045] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Accuracy: 86.248%, Train Accuracy: 84.144%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|████████████████████| 391/391 [02:58<00:00,  2.19it/s, loss=0.00372]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Accuracy: 86.248%, Train Accuracy: 84.552%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|████████████████████| 391/391 [02:58<00:00,  2.18it/s, loss=0.00301]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Accuracy: 86.248%, Train Accuracy: 84.878%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|████████████████████| 391/391 [02:59<00:00,  2.18it/s, loss=0.00238]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Accuracy: 86.248%, Train Accuracy: 85.016%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|████████████████████| 391/391 [02:59<00:00,  2.18it/s, loss=0.00241]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Accuracy: 86.248%, Train Accuracy: 85.152%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|████████████████████| 391/391 [02:59<00:00,  2.18it/s, loss=0.00199]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Accuracy: 86.248%, Train Accuracy: 85.268%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|████████████████████| 391/391 [02:59<00:00,  2.18it/s, loss=0.00175]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Accuracy: 86.248%, Train Accuracy: 85.292%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "epoch_num = 10\n",
    "print(f\"epoch_num: {epoch_num}\")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(f\"Train dataset: {len(train_dataset)}\")\n",
    "print(\"image size: \", train_dataset[0][0].size())\n",
    "\n",
    "# 定义损失函数\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# optimizer_encoder = optim.SGD(encoder.parameters(), lr=1e-3, momentum=0.8, weight_decay=1e-5)\n",
    "# optimizer_decoder = optim.SGD(decoder.parameters(), lr=1e-3, momentum=0.8, weight_decay=1e-5)\n",
    "\n",
    "optimizer_encoder = optim.Adam(encoder.parameters(), lr=1e-4, weight_decay=1e-6)\n",
    "optimizer_decoder = optim.Adam(decoder.parameters(), lr=1e-4, weight_decay=1e-6)\n",
    "\n",
    "model.to(device)\n",
    "conv_segment.to(device)\n",
    "fc_segment.to(device)\n",
    "encoder.to(device)\n",
    "decoder.to(device)\n",
    "\n",
    "model.eval()\n",
    "conv_segment.eval()\n",
    "fc_segment.eval()\n",
    "encoder.train()\n",
    "decoder.train()\n",
    "\n",
    "model.eval()\n",
    "for m in model.modules():\n",
    "    if isinstance(m, nn.BatchNorm2d):\n",
    "        m.track_running_stats = False\n",
    "\n",
    "loss_list = [[] for _ in range(epoch_num)]\n",
    "\n",
    "for epoch in range(epoch_num):\n",
    "    train_loader_tqdm = tqdm(\n",
    "        train_loader,\n",
    "        desc=f\"Epoch {epoch+1}/{epoch_num}\",\n",
    "        bar_format=\"{l_bar}{bar:20}{r_bar}\",\n",
    "    )\n",
    "    correct = 0\n",
    "    correct_truth = 0\n",
    "    total = 0\n",
    "    for images, labels in train_loader_tqdm:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # split image tensor(64, 3, 32, 32) -> [tensor(64, 3, 32, 8) * K]\n",
    "        images_list = []\n",
    "        for _1, _2, start, end in split_data_range:\n",
    "            images_list.append(images[:, :, :, start:end].clone())\n",
    "\n",
    "        pad = (0, 3, 0, 0)\n",
    "        images_list[-1] = F.pad(images_list[-1], pad, \"constant\", value=0)\n",
    "\n",
    "        ground_truth = conv_segment(images)\n",
    "        ground_truth = ground_truth.view(ground_truth.size(0), -1)\n",
    "\n",
    "        # forward\n",
    "        images_list += encoder(images_list)\n",
    "        output_list = []\n",
    "        for i in range(N):\n",
    "            output = conv_segment(images_list[i])\n",
    "            output_list.append(output)\n",
    "        # losed_output_list = lose_something(output_list, self.lose_device_index)\n",
    "        decoded_output_list = decoder(output_list)\n",
    "        output = torch.cat(decoded_output_list, dim=3)\n",
    "        output = output.view(output.size(0), -1)\n",
    "\n",
    "        loss = criterion(output, ground_truth)\n",
    "\n",
    "        loss_list[epoch].append(loss.item())\n",
    "\n",
    "        # backward\n",
    "        optimizer_encoder.zero_grad()\n",
    "        optimizer_decoder.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer_encoder.step()\n",
    "        optimizer_decoder.step()\n",
    "\n",
    "        # calculate accuracy\n",
    "        _, predicted = torch.max(fc_segment(output).data, 1)\n",
    "        _, predicted_truth = torch.max(fc_segment(ground_truth.data), 1)\n",
    "        # print(predicted)\n",
    "        # print(predicted_truth)\n",
    "        # print(labels)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        correct_truth += (predicted_truth == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "        train_loader_tqdm.set_postfix(loss=loss.item())\n",
    "\n",
    "    print(f\"Original Accuracy: {100 * correct_truth / total}%, Train Accuracy: {100 * correct / total}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "保存模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_dir = f\"./save/{TASK_CONFIG['TASK']}/{TASK_CONFIG['MODEL']}/{TASK_CONFIG['DATE']}/K{K}-R{R}-mlp/\"\n",
    "# encoder_path = (\n",
    "#     save_dir\n",
    "#     + f\"encoder-task_{TASK_CONFIG['TASK']}-basemodel_{TASK_CONFIG['MODEL']}-K{K}-R{R}.pth\"\n",
    "# )\n",
    "# decoder_path = (\n",
    "#     save_dir\n",
    "#     + f\"decoder-task_{TASK_CONFIG['TASK']}-basemodel_{TASK_CONFIG['MODEL']}-K{K}-R{R}.pth\"\n",
    "# )\n",
    "\n",
    "# print(f\"save_dir: {save_dir}\")\n",
    "# print(f\"encoder_path: {encoder_path}\")\n",
    "# print(f\"decoder_path: {decoder_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "\n",
    "# os.makedirs(os.path.dirname(encoder_path), exist_ok=True)\n",
    "# os.makedirs(os.path.dirname(decoder_path), exist_ok=True)\n",
    "\n",
    "# torch.save(encoder.state_dict(), encoder_path)\n",
    "# torch.save(decoder.state_dict(), decoder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.15645170211792, 3.5783889293670654, 3.4615845680236816, 3.2651615142822266, 3.3844847679138184, 3.400930166244507, 3.469923973083496, 3.4889750480651855, 3.4052398204803467, 3.526693820953369, 3.149873733520508, 3.3822202682495117, 3.279466152191162, 2.941798210144043, 2.9943928718566895, 2.6153135299682617, 2.8076605796813965, 2.6839263439178467, 2.3214685916900635, 2.0366711616516113, 2.192711353302002, 2.1291747093200684, 2.0355453491210938, 2.0535764694213867, 2.218278169631958, 2.18638014793396, 2.0462303161621094, 1.8303728103637695, 1.8381712436676025, 2.087784767150879, 1.8634135723114014, 1.8777048587799072, 1.86776602268219, 1.8552325963974, 1.8781282901763916, 1.7746882438659668, 2.0732712745666504, 1.8667384386062622, 1.9151195287704468, 1.7484238147735596, 1.9499928951263428, 1.7592861652374268, 1.6632952690124512, 1.7229059934616089, 1.7498080730438232, 1.755472183227539, 1.5873022079467773, 1.8233590126037598, 1.7944351434707642, 1.6369528770446777, 1.6938941478729248, 1.55776047706604, 1.7091619968414307, 1.5187755823135376, 1.7395607233047485, 1.5462864637374878, 1.7077945470809937, 1.5799248218536377, 1.5479273796081543, 1.5739301443099976, 1.5178205966949463, 1.5564002990722656, 1.6140632629394531, 1.5188795328140259, 1.56659734249115, 1.4914606809616089, 1.5321159362792969, 1.3654332160949707, 1.4469022750854492, 1.3514435291290283, 1.260606288909912, 1.410309910774231, 1.4133362770080566, 1.4629790782928467, 1.5209641456604004, 1.4259685277938843, 1.2803479433059692, 1.41510009765625, 1.500570297241211, 1.4792413711547852, 1.3347816467285156, 1.3401038646697998, 1.4199457168579102, 1.360042691230774, 1.2146518230438232, 1.262488842010498, 1.2161548137664795, 1.2621054649353027, 1.166218638420105, 1.2715139389038086, 1.1902642250061035, 1.225987434387207, 1.2217803001403809, 1.2308412790298462, 1.1781470775604248, 1.211262822151184, 1.2951302528381348, 1.10820472240448, 1.132804036140442, 1.1076648235321045, 1.2076301574707031, 1.3470368385314941, 1.1192928552627563, 1.1451144218444824, 1.0501720905303955, 1.2176806926727295, 1.053657054901123, 1.1378285884857178, 1.0529781579971313, 1.1298692226409912, 1.1433920860290527, 1.1718802452087402, 1.2604997158050537, 1.1336987018585205, 1.0147603750228882, 1.0920878648757935, 1.1734516620635986, 1.0260283946990967, 1.063647985458374, 1.1675124168395996, 1.0327637195587158, 1.15266752243042, 1.0741991996765137, 1.1306301355361938, 1.0157945156097412, 1.0536510944366455, 1.070919394493103, 0.9632097482681274, 1.0001119375228882, 1.131399393081665, 1.0446412563323975, 1.1284093856811523, 1.052175760269165, 0.9973413944244385, 0.8890949487686157, 1.0595002174377441, 1.0063838958740234, 1.0106301307678223, 1.0182921886444092, 0.9715081453323364, 1.0321569442749023, 0.9512889981269836, 1.0328006744384766, 1.032332181930542, 0.8727056384086609, 0.9889443516731262, 0.9022676348686218, 0.9719230532646179, 1.0128748416900635, 0.9911145567893982, 1.0352740287780762, 0.9831788539886475, 0.9317675232887268, 0.9877698421478271, 0.9646940231323242, 0.9725772142410278, 0.9806060791015625, 1.0617759227752686, 0.9734171628952026, 0.9789974093437195, 0.9569323658943176, 0.9620864391326904, 0.8880160450935364, 0.993800163269043, 0.9504700899124146, 0.9710687398910522, 0.8509066104888916, 0.8809301257133484, 0.860167920589447, 0.8806968927383423, 1.0577203035354614, 0.9893811941146851, 0.9103975892066956, 0.8544187545776367, 1.0366332530975342, 0.8793131113052368, 0.963912844657898, 0.9926506876945496, 0.8881703615188599, 0.8579335808753967, 0.8242431879043579, 0.9231210947036743, 0.8264562487602234, 0.9014555215835571, 0.9862852692604065, 0.867102324962616, 1.0423696041107178, 0.8810408115386963, 0.8911787271499634, 0.7828160524368286, 0.9112293124198914, 0.9115773439407349, 0.7760615944862366, 0.8244062066078186, 0.9171563386917114, 0.8754103183746338, 0.7437613010406494, 0.8583776950836182, 0.7484121918678284, 0.8416199088096619, 0.9359922409057617, 0.9343513250350952, 0.9105945825576782, 0.8284798860549927, 0.772026777267456, 0.927097737789154, 0.9382219314575195, 0.9227986335754395, 0.834915280342102, 0.8151277303695679, 0.7536511421203613, 0.8335341215133667, 0.8810064196586609, 0.7662025690078735, 0.7655208706855774, 0.8799781799316406, 0.7803664207458496, 0.6972063779830933, 0.7629835605621338, 0.8066539764404297, 0.8142577409744263, 0.742831826210022, 0.7363839745521545, 0.7612369656562805, 0.7630406618118286, 0.7490260004997253, 0.7395695447921753, 0.8551626205444336, 0.8607776165008545, 0.751897394657135, 0.8798108100891113, 0.7733453512191772, 0.7675443887710571, 0.74027419090271, 0.8246346712112427, 0.7956957221031189, 0.7808601260185242, 0.8019125461578369, 0.7922753095626831, 0.760430097579956, 0.7750202417373657, 0.8356203436851501, 0.7737547159194946, 0.8283386826515198, 0.7713027000427246, 0.692852258682251, 0.7634884715080261, 0.7275458574295044, 0.8103545904159546, 0.7375872731208801, 0.839638352394104, 0.8361824750900269, 0.7324624061584473, 0.7595327496528625, 0.759032666683197, 0.7639824748039246, 0.7104357481002808, 0.7296827435493469, 0.8014336228370667, 0.6197930574417114, 0.726966381072998, 0.7153097987174988, 0.6938966512680054, 0.7169108390808105, 0.7385269999504089, 0.7507621645927429, 0.7827337384223938, 0.7636064887046814, 0.7472754716873169, 0.7375550270080566, 0.7624240517616272, 0.6446654796600342, 0.7221397757530212, 0.6752649545669556, 0.7323470711708069, 0.7802009582519531, 0.6656118631362915, 0.7884159088134766, 0.8084033727645874, 0.683918833732605, 0.769781231880188, 0.8012155294418335, 0.7400468587875366, 0.6695338487625122, 0.6566917896270752, 0.7473241686820984, 0.7810107469558716, 0.7315379977226257, 0.8082229495048523, 0.6917484402656555, 0.6995750665664673, 0.706771969795227, 0.7472386360168457, 0.7276628613471985, 0.742595911026001, 0.6551348567008972, 0.7260916829109192, 0.7694581747055054, 0.6829878091812134, 0.7894362211227417, 0.76259446144104, 0.7815477848052979, 0.6995456218719482, 0.6891108751296997, 0.72527015209198, 0.8090533018112183, 0.6709228157997131, 0.6684664487838745, 0.7181664109230042, 0.6343764066696167, 0.7070488929748535, 0.75104159116745, 0.7237656712532043, 0.8021718263626099, 0.6851895451545715, 0.7029502391815186, 0.6780599355697632, 0.7356189489364624, 0.6397652626037598, 0.6899440288543701, 0.7131507396697998, 0.6159201860427856, 0.7469120025634766, 0.7207497954368591, 0.7741947770118713, 0.6781835556030273, 0.6847005486488342, 0.6620296239852905, 0.717362642288208, 0.631069540977478, 0.7125405073165894, 0.728304386138916, 0.6637354493141174, 0.7096045613288879, 0.560249388217926, 0.7329623699188232, 0.6761558055877686, 0.7167941331863403, 0.6692959070205688, 0.6760691404342651, 0.698019802570343, 0.6182573437690735, 0.7366460561752319, 0.709571123123169, 0.630098819732666, 0.6281817555427551, 0.7598445415496826, 0.6370978355407715, 0.6785492897033691, 0.7318867444992065, 0.7082103490829468, 0.5643125176429749, 0.6862162947654724, 0.6784217357635498, 0.622060239315033, 0.694652795791626, 0.6785233616828918, 0.7627053260803223, 0.6779572367668152, 0.5791200399398804, 0.6630411148071289, 0.6527996063232422, 0.6488759517669678, 0.6465772390365601, 0.6793311834335327, 0.6935363411903381, 0.6039042472839355, 0.7437148094177246, 0.6568745970726013, 0.6409847140312195, 0.7300150990486145, 0.589652419090271, 0.6553019285202026, 0.6367881894111633, 0.6678071618080139, 0.7294052839279175, 0.7085210084915161, 0.6975299119949341, 0.6904414296150208, 0.5780822038650513, 0.6344246864318848, 0.5709660053253174, 0.6711640357971191, 0.60162353515625, 0.705708920955658, 0.6710284948348999, 0.5652350187301636, 0.5749474763870239, 0.6097065806388855, 0.668239951133728, 0.6418593525886536, 0.5677536725997925, 0.7471189498901367, 0.5627934336662292, 0.6077253818511963, 0.5703555345535278, 0.5383312702178955, 0.5739821195602417, 0.6112061142921448, 0.5632205605506897, 0.617893397808075, 0.7097412943840027, 0.5522669553756714, 0.6282575130462646, 0.6707653403282166, 0.5744508504867554, 0.6491154432296753, 0.6243348121643066, 0.6256108283996582, 0.6347177624702454, 0.554989218711853, 0.5533853769302368, 0.5451520085334778, 0.6420236825942993, 0.6254997253417969, 0.6184974908828735, 0.5852293968200684, 0.5398044586181641, 0.6240345239639282, 0.5931764841079712, 0.6530032157897949, 0.5866274833679199, 0.577156662940979, 0.5655391812324524, 0.625173807144165, 0.45254644751548767, 0.578909695148468, 0.5403368473052979, 0.559329628944397, 0.5908553600311279, 0.6884624361991882, 0.5364398956298828, 0.5922673344612122, 0.5629630088806152, 0.6393227577209473, 0.6820982694625854, 0.5990374088287354, 0.5275578498840332, 0.5914638042449951, 0.6196067333221436, 0.569888174533844, 0.6042661666870117, 0.54994797706604, 0.5465704202651978, 0.6199325323104858, 0.5454099774360657, 0.5449743270874023, 0.5335782766342163, 0.6445754766464233, 0.5894760489463806, 0.5361300706863403, 0.5851395130157471, 0.5627752542495728, 0.5023500919342041, 0.5632117390632629, 0.605832576751709, 0.6344164609909058, 0.6002354621887207, 0.630324125289917, 0.6374936699867249, 0.5662795305252075, 0.5076568126678467, 0.6332346200942993, 0.5527737140655518, 0.5709691643714905, 0.5720942616462708, 0.6471648216247559, 0.5933287739753723, 0.4964851438999176, 0.5610566139221191, 0.6002341508865356, 0.5625487565994263, 0.5187658071517944, 0.607292890548706, 0.5579060316085815, 0.579262375831604, 0.6431623697280884, 0.5342551469802856, 0.5804930329322815, 0.6044853925704956, 0.5468138456344604, 0.5543034076690674, 0.5467926859855652, 0.5587792992591858, 0.5545481443405151, 0.6004456281661987, 0.5229325890541077, 0.5583674907684326, 0.574508547782898, 0.528586745262146, 0.5751316547393799, 0.5482032299041748, 0.5443395972251892, 0.5846961736679077, 0.5170474052429199, 0.48871758580207825, 0.560265064239502, 0.5476220846176147, 0.5405595898628235, 0.5414572358131409, 0.5245841145515442, 0.5013500452041626, 0.5351435542106628, 0.5363432765007019, 0.5586201548576355, 0.5626229047775269, 0.6443628072738647, 0.5084986686706543, 0.5459984540939331, 0.5317312479019165, 0.6145035028457642, 0.6112902164459229, 0.5885370969772339, 0.528812050819397, 0.5003772974014282, 0.533663809299469, 0.6244228482246399, 0.507463812828064, 0.5262283682823181, 0.5036325454711914, 0.5233675241470337, 0.5427975654602051, 0.5783023238182068, 0.5334084033966064, 0.5450445413589478, 0.5650932788848877, 0.49437886476516724, 0.6003144979476929, 0.5293847918510437, 0.5801963210105896, 0.5239449143409729, 0.511591911315918, 0.4975799024105072, 0.5643230080604553, 0.5504384636878967, 0.5604406595230103, 0.5159018039703369, 0.5432549715042114, 0.5244965553283691, 0.5556433200836182, 0.5154820680618286, 0.5198134779930115, 0.5268247127532959, 0.5209577083587646, 0.5376657843589783, 0.5134588479995728, 0.5869510173797607, 0.5277815461158752, 0.5425657033920288, 0.5229721069335938, 0.5182855129241943, 0.46532177925109863, 0.537872314453125, 0.5636364221572876, 0.4627585709095001, 0.5244607925415039, 0.5847555994987488, 0.6017698049545288, 0.47015810012817383, 0.5313233137130737, 0.5442094802856445, 0.47003230452537537, 0.5080800652503967, 0.5233092904090881, 0.4306972026824951, 0.5671563148498535, 0.49439874291419983, 0.541302502155304, 0.5631481409072876, 0.5114954113960266, 0.4449860453605652, 0.5192257165908813, 0.4828077554702759, 0.4999867081642151, 0.5228395462036133, 0.4830666780471802, 0.5078121423721313, 0.5109741687774658, 0.5196835994720459, 0.46648532152175903, 0.5018847584724426, 0.49694299697875977, 0.5329713821411133, 0.5313583612442017, 0.45724359154701233, 0.5500585436820984, 0.4678344428539276, 0.5087304711341858, 0.5096920132637024, 0.5767039060592651, 0.5476319789886475, 0.5484176874160767, 0.5147610902786255, 0.4573374390602112, 0.46634441614151, 0.5119357109069824, 0.48409304022789, 0.49582719802856445, 0.47172898054122925, 0.5107085704803467, 0.5287931561470032, 0.5565040707588196, 0.5048508048057556, 0.5170616507530212, 0.4703165292739868, 0.5024206042289734, 0.5300315618515015, 0.5347982048988342, 0.551173746585846, 0.5278980731964111, 0.5151312351226807, 0.46746355295181274, 0.5263532400131226, 0.4660804271697998, 0.5444284677505493, 0.5287795066833496, 0.46222180128097534, 0.46363741159439087, 0.518096923828125, 0.47066131234169006, 0.5183025598526001, 0.5852545499801636, 0.49476027488708496, 0.4330517053604126, 0.5240264534950256, 0.44951725006103516, 0.5394223928451538, 0.5122538208961487, 0.5038336515426636, 0.46043288707733154, 0.4795972406864166, 0.477371484041214, 0.4791370630264282, 0.48594731092453003, 0.4979119300842285, 0.5068587064743042, 0.5105960369110107, 0.47402021288871765, 0.49479758739471436, 0.5014209151268005, 0.4558403491973877, 0.4457305073738098, 0.49595579504966736, 0.4826471209526062, 0.4702322483062744, 0.46702539920806885, 0.474038302898407, 0.5237342119216919, 0.4775956869125366, 0.4787435531616211, 0.49892711639404297, 0.5134494304656982, 0.45930618047714233, 0.533797562122345, 0.4996325969696045, 0.5061953663825989, 0.5249348878860474, 0.4778083562850952, 0.46375441551208496, 0.4645855128765106, 0.4700454771518707, 0.4898725152015686, 0.4567854404449463, 0.5235661268234253, 0.4806059002876282, 0.5297900438308716, 0.47882911562919617, 0.44230812788009644, 0.4522063732147217, 0.4684251546859741, 0.49944955110549927, 0.4519544839859009, 0.4600229859352112, 0.47078993916511536, 0.40684688091278076, 0.4870017468929291, 0.4354591965675354, 0.5046377182006836, 0.47798216342926025, 0.4666967988014221, 0.4553942084312439, 0.5127651691436768, 0.49082913994789124, 0.4474295973777771, 0.5086349248886108, 0.42460131645202637, 0.4793756902217865, 0.4692590534687042, 0.47532132267951965, 0.476889967918396, 0.43927890062332153, 0.4508768320083618, 0.47771963477134705, 0.44241097569465637, 0.4530891478061676, 0.4244932532310486, 0.48787805438041687, 0.4918275773525238, 0.4475940465927124, 0.4688165783882141, 0.4337197542190552, 0.4845840334892273, 0.527225911617279, 0.43842780590057373, 0.5083078742027283, 0.4297083020210266, 0.4970359802246094, 0.43214020133018494, 0.48286011815071106, 0.45975708961486816, 0.5053188800811768, 0.48243248462677, 0.4930376708507538, 0.46250659227371216, 0.47674670815467834, 0.4268529415130615, 0.4895116090774536, 0.40977323055267334, 0.48008131980895996, 0.49881410598754883, 0.510830283164978, 0.4440884590148926, 0.4361845552921295, 0.4728373885154724, 0.4797323942184448, 0.4348151981830597, 0.4320915937423706, 0.4314635097980499, 0.45230597257614136, 0.5140501260757446, 0.499369353055954, 0.4509347379207611, 0.5095397233963013, 0.4803612530231476, 0.45809105038642883, 0.45143386721611023, 0.4644007682800293, 0.512837290763855, 0.4297926723957062, 0.44101375341415405, 0.46810823678970337, 0.5336461067199707, 0.41574347019195557, 0.4204983711242676, 0.44632992148399353, 0.4235207438468933, 0.4132503569126129, 0.4454694092273712, 0.45173245668411255, 0.4678957462310791, 0.4540201425552368, 0.4512023627758026, 0.5057483911514282, 0.46157407760620117, 0.4543353021144867, 0.3928586542606354, 0.39642322063446045, 0.4126485586166382, 0.4930451512336731, 0.43144118785858154, 0.4040508270263672, 0.4724133610725403, 0.4305664300918579, 0.434579461812973, 0.4995550811290741, 0.48260098695755005, 0.4020657241344452, 0.40538904070854187, 0.5037598013877869, 0.3638151288032532, 0.4251749515533447, 0.4344435930252075, 0.47326862812042236, 0.4523191750049591, 0.3925643563270569, 0.4467749297618866, 0.44003826379776, 0.46152377128601074, 0.4724293351173401, 0.44946837425231934, 0.42859333753585815, 0.45771244168281555, 0.43562179803848267, 0.41633614897727966, 0.4149913787841797, 0.4690466523170471, 0.44976532459259033, 0.43489211797714233, 0.45242980122566223, 0.4148252010345459, 0.4228970408439636, 0.4673609137535095, 0.4503815174102783, 0.38043850660324097, 0.391851007938385, 0.44366902112960815, 0.4019651412963867, 0.44908010959625244, 0.47553208470344543, 0.41816508769989014, 0.4190494120121002, 0.42354443669319153, 0.41239434480667114, 0.3863518238067627, 0.4518061876296997, 0.42429882287979126, 0.4506058692932129, 0.42132169008255005, 0.3820306360721588, 0.4496263265609741, 0.4636710286140442, 0.43057781457901, 0.4426805377006531, 0.37016063928604126, 0.44575750827789307, 0.39268332719802856, 0.4327840805053711, 0.4259054660797119, 0.4848228991031647, 0.43459612131118774, 0.4742316007614136, 0.42813724279403687, 0.44225507974624634, 0.4575720727443695, 0.42453622817993164, 0.388618141412735, 0.395538866519928, 0.4038482904434204, 0.4006401300430298, 0.42764756083488464, 0.4191662669181824, 0.3508707880973816, 0.3930899202823639, 0.4966605305671692, 0.39347130060195923, 0.3906938135623932, 0.35075026750564575, 0.47554272413253784, 0.40322360396385193, 0.40781843662261963, 0.4497283697128296, 0.377946138381958, 0.42170479893684387, 0.4663047790527344, 0.44607287645339966, 0.4700809121131897, 0.5132734775543213, 0.42580646276474, 0.425007700920105, 0.44295433163642883, 0.39078664779663086, 0.448850154876709, 0.374910831451416, 0.4880121350288391, 0.41473105549812317, 0.40319740772247314, 0.39740946888923645, 0.39241474866867065, 0.4513508081436157, 0.4199167490005493, 0.4257615804672241, 0.4020765721797943, 0.40586721897125244, 0.41360342502593994, 0.4648117423057556, 0.3931019604206085, 0.39058828353881836, 0.38857924938201904, 0.4512747526168823, 0.3899041414260864, 0.4344469904899597, 0.41212111711502075, 0.42390984296798706, 0.4364289939403534, 0.3863909840583801, 0.4017094373703003, 0.42821383476257324, 0.37919604778289795, 0.3880683183670044, 0.36693280935287476, 0.3904253840446472, 0.3917410969734192, 0.3351706862449646, 0.3850246071815491, 0.4024815559387207, 0.3676854968070984, 0.3886618912220001, 0.4017772376537323, 0.3875207304954529, 0.46028417348861694, 0.41866201162338257, 0.4074026346206665, 0.40333518385887146, 0.4109432101249695, 0.44324222207069397, 0.4095413088798523, 0.41398194432258606, 0.4052574336528778, 0.4427627623081207, 0.4051935076713562, 0.353021502494812, 0.410208523273468, 0.4045078456401825, 0.41089877486228943, 0.41983354091644287, 0.4422079622745514, 0.4384628236293793, 0.35489845275878906, 0.36274170875549316, 0.4020920991897583, 0.4527016580104828, 0.3897716999053955, 0.4114246964454651, 0.47133001685142517, 0.36715906858444214, 0.44201162457466125, 0.4267684519290924, 0.39517301321029663, 0.350419819355011, 0.38544711470603943, 0.3769473731517792, 0.47503113746643066, 0.46389687061309814, 0.40901464223861694, 0.4308517873287201, 0.4068704843521118, 0.36675167083740234, 0.43044769763946533, 0.3674577474594116, 0.3778781592845917, 0.41761231422424316, 0.40268248319625854, 0.42197203636169434, 0.39495354890823364, 0.46798771619796753, 0.40462616086006165, 0.43526944518089294, 0.45930957794189453] [0.3949055075645447, 0.4546630382537842, 0.34555956721305847, 0.3999134302139282, 0.3693496584892273, 0.3410680890083313, 0.4281359314918518, 0.4501689374446869, 0.4126330614089966, 0.37950778007507324, 0.42817923426628113, 0.3825191259384155, 0.4062095582485199, 0.39289623498916626, 0.4013313949108124, 0.3866848647594452, 0.40079012513160706, 0.37904977798461914, 0.36617544293403625, 0.3732116222381592, 0.3916768431663513, 0.44051066040992737, 0.3877415060997009, 0.42000019550323486, 0.40685853362083435, 0.3895595669746399, 0.3414958715438843, 0.4070870280265808, 0.37896671891212463, 0.3166247606277466, 0.3540857434272766, 0.4060097336769104, 0.44698312878608704, 0.39866942167282104, 0.411764919757843, 0.3777124285697937, 0.3823241591453552, 0.3710513710975647, 0.4275254011154175, 0.34029334783554077, 0.41750568151474, 0.447392076253891, 0.37618640065193176, 0.38031554222106934, 0.36955809593200684, 0.41783803701400757, 0.4120703339576721, 0.4142289161682129, 0.3946807086467743, 0.36057963967323303, 0.3782014846801758, 0.42225968837738037, 0.3392484784126282, 0.35038328170776367, 0.36433106660842896, 0.3368869721889496, 0.381885826587677, 0.37774139642715454, 0.3606206476688385, 0.44904521107673645, 0.37143537402153015, 0.36103859543800354, 0.39201486110687256, 0.3242708742618561, 0.42389416694641113, 0.3853510320186615, 0.3610001802444458, 0.3596847653388977, 0.35990110039711, 0.41229063272476196, 0.3714057207107544, 0.43978407979011536, 0.4148167371749878, 0.35625362396240234, 0.37452036142349243, 0.35510319471359253, 0.3984237313270569, 0.3801388740539551, 0.37040263414382935, 0.37402963638305664, 0.3812252879142761, 0.38307225704193115, 0.374874472618103, 0.39659878611564636, 0.40427452325820923, 0.39414161443710327, 0.3839605152606964, 0.35332944989204407, 0.41430580615997314, 0.3977753520011902, 0.4132286310195923, 0.3803422451019287, 0.3972708582878113, 0.3928804099559784, 0.4003788232803345, 0.3740912973880768, 0.3852681815624237, 0.38782399892807007, 0.4182245135307312, 0.39459386467933655, 0.4255490303039551, 0.49355652928352356, 0.36635273694992065, 0.381632924079895, 0.33067771792411804, 0.3789445161819458, 0.3624442517757416, 0.3939066529273987, 0.3443140983581543, 0.34057775139808655, 0.35259464383125305, 0.4048503637313843, 0.33974605798721313, 0.3843719959259033, 0.35110801458358765, 0.3774312734603882, 0.3305293321609497, 0.39822152256965637, 0.3684413433074951, 0.3405771851539612, 0.36970311403274536, 0.4095619320869446, 0.3664949834346771, 0.31989240646362305, 0.40500372648239136, 0.371319979429245, 0.3772137761116028, 0.40039241313934326, 0.40603020787239075, 0.3791719377040863, 0.37671035528182983, 0.39877110719680786, 0.2973628640174866, 0.4177689552307129, 0.35926342010498047, 0.33087149262428284, 0.3135608434677124, 0.35539624094963074, 0.3440002202987671, 0.30585604906082153, 0.3682714104652405, 0.38641902804374695, 0.34018605947494507, 0.3101347088813782, 0.36536097526550293, 0.3570854067802429, 0.3614014983177185, 0.3941103219985962, 0.334600567817688, 0.32403135299682617, 0.38624483346939087, 0.3556610345840454, 0.3813309669494629, 0.31483256816864014, 0.35465043783187866, 0.3707529902458191, 0.40809476375579834, 0.40512388944625854, 0.3680979013442993, 0.3584246039390564, 0.3989880681037903, 0.38335421681404114, 0.36671918630599976, 0.39451703429222107, 0.3821676969528198, 0.3704463243484497, 0.3419528007507324, 0.40227192640304565, 0.3654288649559021, 0.3874521851539612, 0.3488461971282959, 0.31837141513824463, 0.29862284660339355, 0.44700807332992554, 0.3574580252170563, 0.3555341064929962, 0.386762797832489, 0.3648211359977722, 0.3491422235965729, 0.38223204016685486, 0.3529554605484009, 0.3645191788673401, 0.31929004192352295, 0.3247808814048767, 0.3282044529914856, 0.33855393528938293, 0.38237515091896057, 0.3984561562538147, 0.3573235869407654, 0.34359896183013916, 0.3466119170188904, 0.3257564306259155, 0.3616951107978821, 0.33336785435676575, 0.37234604358673096, 0.35411179065704346, 0.40296632051467896, 0.34576350450515747, 0.3348045349121094, 0.3592722415924072, 0.35335573554039, 0.3295372724533081, 0.3702426552772522, 0.39153096079826355, 0.33585691452026367, 0.3725718557834625, 0.3426886200904846, 0.3600049614906311, 0.37639105319976807, 0.3677074909210205, 0.35015669465065, 0.3691304922103882, 0.3597562611103058, 0.31955423951148987, 0.3146875500679016, 0.3158784806728363, 0.2982047200202942, 0.3389797508716583, 0.3666209876537323, 0.41275128722190857, 0.3535382151603699, 0.413158655166626, 0.3385735750198364, 0.3326440453529358, 0.37409156560897827, 0.36967092752456665, 0.38227665424346924, 0.3458109498023987, 0.31521984934806824, 0.3757208287715912, 0.3354230523109436, 0.3689766824245453, 0.3459928631782532, 0.34763070940971375, 0.390468567609787, 0.36517027020454407, 0.386735200881958, 0.4056687653064728, 0.38528335094451904, 0.3271215558052063, 0.33850547671318054, 0.36685314774513245, 0.37226036190986633, 0.37394362688064575, 0.3158071041107178, 0.348394513130188, 0.34720706939697266, 0.3498389720916748, 0.3959970474243164, 0.32200151681900024, 0.30523696541786194, 0.3417506814002991, 0.3613114058971405, 0.31668153405189514, 0.3229471743106842, 0.3272871971130371, 0.3421211540699005, 0.3622511029243469, 0.3282667398452759, 0.3609400987625122, 0.37993955612182617, 0.35681551694869995, 0.32688769698143005, 0.328734815120697, 0.33094581961631775, 0.3795684576034546, 0.3506302833557129, 0.34916913509368896, 0.32254648208618164, 0.3414115905761719, 0.32061243057250977, 0.34365642070770264, 0.33878129720687866, 0.378383994102478, 0.33927762508392334, 0.32398831844329834, 0.3380333185195923, 0.30901551246643066, 0.37806981801986694, 0.35536158084869385, 0.3661573529243469, 0.32840636372566223, 0.37428683042526245, 0.39937645196914673, 0.3131836950778961, 0.361948162317276, 0.33160239458084106, 0.3575916886329651, 0.31744664907455444, 0.337545245885849, 0.3399208188056946, 0.35354146361351013, 0.3280114531517029, 0.3212405741214752, 0.36080247163772583, 0.35046955943107605, 0.3574555516242981, 0.29109635949134827, 0.3487139642238617, 0.3749144673347473, 0.34280771017074585, 0.39164766669273376, 0.35074299573898315, 0.33757203817367554, 0.3439396023750305, 0.3520122766494751, 0.34808558225631714, 0.3267337381839752, 0.335548460483551, 0.33491069078445435, 0.3320557475090027, 0.3943895399570465, 0.36510831117630005, 0.3053327202796936, 0.30954885482788086, 0.33796048164367676, 0.3369632363319397, 0.30426985025405884, 0.35101231932640076, 0.31451845169067383, 0.3077108860015869, 0.30265581607818604, 0.3288809657096863, 0.33791613578796387, 0.31880009174346924, 0.32070350646972656, 0.3074018955230713, 0.329059362411499, 0.3444886803627014, 0.34829944372177124, 0.3188752233982086, 0.36510419845581055, 0.3266637623310089, 0.321006178855896, 0.33960020542144775, 0.36902257800102234, 0.38681989908218384, 0.3167676329612732, 0.29678598046302795, 0.32371920347213745, 0.34349411725997925, 0.3564680218696594, 0.3305429220199585, 0.3374427556991577, 0.3168347477912903, 0.33463621139526367, 0.39845582842826843, 0.29289987683296204, 0.31316033005714417, 0.3691515326499939, 0.29317301511764526, 0.31643155217170715, 0.3502650260925293, 0.29451197385787964, 0.30834639072418213, 0.3431517481803894, 0.32149219512939453, 0.3589653968811035, 0.33560842275619507, 0.3107650578022003, 0.3571023941040039, 0.3117295503616333, 0.29961201548576355, 0.33329468965530396, 0.3004155158996582, 0.28164923191070557, 0.2871854901313782, 0.36018607020378113, 0.3247961103916168, 0.2941333055496216, 0.3175870180130005, 0.3210529386997223, 0.3320612907409668, 0.2933131456375122, 0.3550354838371277, 0.3516550660133362, 0.31683841347694397, 0.29967713356018066, 0.40585172176361084, 0.31929725408554077, 0.3582072854042053, 0.35618287324905396, 0.2959288954734802, 0.34868693351745605, 0.3720579147338867, 0.29882192611694336, 0.3458137512207031, 0.3387642204761505, 0.3239240050315857, 0.35922521352767944, 0.35022950172424316, 0.3763478696346283, 0.35026925802230835, 0.3458777964115143, 0.32658669352531433, 0.3501761555671692, 0.3265540599822998, 0.32123512029647827, 0.2980419993400574, 0.307801216840744, 0.32323330640792847, 0.3522515892982483, 0.32862532138824463, 0.3259659707546234, 0.31174951791763306, 0.3607611060142517, 0.3560492992401123, 0.3403344750404358, 0.3339427411556244, 0.3086089491844177, 0.3858189582824707, 0.32028040289878845, 0.30829793214797974, 0.31495124101638794, 0.32524192333221436, 0.34678274393081665, 0.29282915592193604, 0.3201102018356323, 0.28715941309928894, 0.38902559876441956, 0.3391343057155609, 0.3477007746696472, 0.30204078555107117, 0.29677557945251465, 0.32074493169784546, 0.33927014470100403, 0.316569983959198, 0.3406280279159546, 0.3002605438232422, 0.28414851427078247, 0.3359262943267822, 0.3181852102279663, 0.30911099910736084, 0.27535581588745117, 0.32832056283950806, 0.33588314056396484, 0.30505818128585815, 0.34345871210098267, 0.37526553869247437, 0.32864975929260254, 0.36324912309646606, 0.33234086632728577, 0.2940726578235626, 0.2919049561023712, 0.3622470796108246, 0.30503299832344055, 0.34520232677459717, 0.29795998334884644, 0.32061633467674255, 0.3094896078109741, 0.3289232552051544, 0.3276086449623108, 0.3049179017543793, 0.29673898220062256, 0.3218397796154022, 0.3286873698234558, 0.3764059543609619, 0.30903542041778564, 0.3450382351875305, 0.2960891127586365, 0.32110387086868286, 0.3556180000305176, 0.3181092143058777, 0.3112124800682068, 0.31559979915618896, 0.27899789810180664, 0.3175853490829468, 0.32026371359825134, 0.3073514699935913, 0.275243878364563, 0.30502849817276, 0.3018544316291809, 0.28213340044021606, 0.32740068435668945, 0.2860516905784607, 0.3360251784324646, 0.3256548047065735, 0.35931479930877686, 0.32465091347694397, 0.2921239733695984, 0.29428306221961975, 0.27949759364128113, 0.31913501024246216, 0.30388933420181274, 0.2977284789085388, 0.3169710040092468, 0.32797807455062866, 0.324191689491272, 0.26393628120422363, 0.2938058376312256, 0.29086655378341675, 0.3330380916595459, 0.3236733078956604, 0.28522545099258423, 0.3183334469795227, 0.31225553154945374, 0.33293524384498596, 0.3180646300315857, 0.33348411321640015, 0.34167471528053284, 0.3145846426486969, 0.34763067960739136, 0.2744910418987274, 0.2915162742137909, 0.3308486044406891, 0.32906976342201233, 0.3325020670890808, 0.317118376493454, 0.3212079107761383, 0.28197771310806274, 0.2621306777000427, 0.3021995425224304, 0.3023775815963745, 0.30459409952163696, 0.32639777660369873, 0.32437145709991455, 0.2906774580478668, 0.3117050528526306, 0.2781342566013336, 0.26988542079925537, 0.3132018744945526, 0.3154310882091522, 0.32385575771331787, 0.2915131151676178, 0.2872117757797241, 0.29696595668792725, 0.309272438287735, 0.29656654596328735, 0.29314354062080383, 0.2743498682975769, 0.32491880655288696, 0.2716214060783386, 0.26625341176986694, 0.31758612394332886, 0.30085402727127075, 0.3066915273666382, 0.31719666719436646, 0.33366331458091736, 0.2912063002586365, 0.2974719703197479, 0.30262720584869385, 0.3196603059768677, 0.2755948305130005, 0.273298442363739, 0.3210553824901581, 0.28511950373649597, 0.30068135261535645, 0.3251333236694336, 0.3410096764564514, 0.3232562243938446, 0.2743876874446869, 0.3224845230579376, 0.2976490557193756, 0.3361204266548157, 0.3385885953903198, 0.2985686659812927, 0.30272454023361206, 0.2901853024959564, 0.266290545463562, 0.3190399408340454, 0.31595006585121155, 0.27227845788002014, 0.30631986260414124, 0.2502680718898773, 0.33016812801361084, 0.300678551197052, 0.3322911262512207, 0.29993653297424316, 0.30081993341445923, 0.3131541609764099, 0.2575419545173645, 0.2682986855506897, 0.33096903562545776, 0.3310244679450989, 0.33343076705932617, 0.26750099658966064, 0.2736356854438782, 0.32230305671691895, 0.2957553267478943, 0.28991636633872986, 0.32864275574684143, 0.321280837059021, 0.34054863452911377, 0.3029891550540924, 0.3172468841075897, 0.2786015272140503, 0.2856815457344055, 0.29879075288772583, 0.2775135934352875, 0.2891307473182678, 0.2972780466079712, 0.3016478717327118, 0.2823929786682129, 0.2686428129673004, 0.2989603877067566, 0.2966943383216858, 0.32429277896881104, 0.2842668890953064, 0.2893764078617096, 0.3063359558582306, 0.30985087156295776, 0.30370768904685974, 0.30059969425201416, 0.3067737817764282, 0.287794291973114, 0.23071423172950745, 0.3036003112792969, 0.2873304486274719, 0.3647664189338684, 0.30375710129737854, 0.32043740153312683, 0.26316651701927185, 0.3062083125114441, 0.29636597633361816, 0.32528436183929443, 0.30903708934783936, 0.26261475682258606, 0.2813046872615814, 0.32915258407592773, 0.2900918424129486, 0.3023330271244049, 0.32340407371520996, 0.29035717248916626, 0.31382912397384644, 0.3043960928916931, 0.2993989586830139, 0.2940777540206909, 0.2611585855484009, 0.2732103168964386, 0.29666274785995483, 0.29783642292022705, 0.30032891035079956, 0.27632391452789307, 0.27641361951828003, 0.27187955379486084, 0.306627094745636, 0.37775182723999023, 0.32290494441986084, 0.2932218313217163, 0.2802448868751526, 0.25222957134246826, 0.25015854835510254, 0.30137014389038086, 0.2731935977935791, 0.27755025029182434, 0.2683412730693817, 0.3198544383049011, 0.26983124017715454, 0.2741127014160156, 0.3282282054424286, 0.35755211114883423, 0.2849954664707184, 0.308202862739563, 0.28085267543792725, 0.3209530711174011, 0.29392582178115845, 0.3023344576358795, 0.28663957118988037, 0.30779266357421875, 0.28585895895957947, 0.2768850326538086, 0.30382853746414185, 0.26225990056991577, 0.2521437406539917, 0.3356439769268036, 0.32692164182662964, 0.2899407148361206, 0.2793233096599579, 0.3359850347042084, 0.2956005930900574, 0.29757970571517944, 0.2921411991119385, 0.2818422317504883, 0.3746781647205353, 0.30294063687324524, 0.2930125594139099, 0.3037582039833069, 0.321249783039093, 0.2912391424179077, 0.3087218403816223, 0.3532169461250305, 0.29272061586380005, 0.2803427577018738, 0.31048357486724854, 0.3025168478488922, 0.2688983678817749, 0.2946441173553467, 0.2755736708641052, 0.28376123309135437, 0.333163321018219, 0.30416974425315857, 0.3160705864429474, 0.26584044098854065, 0.36149853467941284, 0.29700589179992676, 0.27242177724838257, 0.3009101450443268, 0.29653382301330566, 0.31515759229660034, 0.24057050049304962, 0.30821526050567627, 0.2816622853279114, 0.2980318069458008, 0.2814665734767914, 0.3250836133956909, 0.29512155055999756, 0.31608259677886963, 0.31336352229118347, 0.3050692677497864, 0.30114611983299255, 0.2832479476928711, 0.31203019618988037, 0.30945998430252075, 0.28365087509155273, 0.28346946835517883, 0.2934996783733368, 0.32734811305999756, 0.277987539768219, 0.3011090159416199, 0.2861674427986145, 0.28820565342903137, 0.2831832468509674, 0.29154491424560547, 0.31720659136772156, 0.2742423713207245, 0.33293241262435913, 0.27548515796661377, 0.2979884743690491, 0.21441692113876343, 0.29941391944885254, 0.2905438542366028, 0.30315345525741577, 0.2759757936000824, 0.26148396730422974, 0.26631641387939453, 0.29563236236572266, 0.3205811381340027, 0.26403188705444336, 0.2918347418308258, 0.2799645662307739, 0.2819328308105469, 0.30063891410827637, 0.30052614212036133, 0.248490571975708, 0.3085692524909973, 0.24992980062961578, 0.26300954818725586, 0.2828519344329834, 0.2885531783103943, 0.34546875953674316, 0.30899864435195923, 0.2575860619544983, 0.2684457004070282, 0.28554731607437134, 0.29193687438964844, 0.2900005877017975, 0.2986738979816437, 0.28163793683052063, 0.2804923951625824, 0.2988875210285187, 0.28317636251449585, 0.2857464551925659, 0.30293363332748413, 0.3343699872493744, 0.28723883628845215, 0.2684863805770874, 0.24628381431102753, 0.3235067129135132, 0.2854357361793518, 0.33721572160720825, 0.27340200543403625, 0.2628767490386963, 0.28716665506362915, 0.3071504831314087, 0.2781854271888733, 0.31176406145095825, 0.2674090266227722, 0.2864217758178711, 0.28972429037094116, 0.23687590658664703, 0.2893356382846832, 0.28569966554641724, 0.29866740107536316, 0.28098246455192566, 0.30769219994544983, 0.2827506959438324, 0.286759614944458, 0.30205678939819336, 0.3089022934436798, 0.3125145435333252, 0.274019718170166, 0.28967076539993286, 0.26778966188430786, 0.27857938408851624, 0.29387080669403076, 0.31092411279678345, 0.2618924379348755, 0.27375954389572144, 0.31073856353759766, 0.26407185196876526, 0.25798413157463074, 0.2568807303905487, 0.3413976728916168, 0.2780168056488037, 0.30277350544929504, 0.25301507115364075, 0.27587229013442993, 0.3452983498573303, 0.30867430567741394, 0.30227041244506836, 0.2942093014717102, 0.27883216738700867, 0.2926322817802429, 0.2861878275871277, 0.3078609108924866, 0.29996374249458313, 0.2770109176635742, 0.2935851216316223, 0.3076063096523285, 0.33676081895828247, 0.2663725018501282, 0.31214505434036255, 0.271603524684906, 0.24942632019519806, 0.28025591373443604, 0.2761942446231842, 0.2582162916660309, 0.27765727043151855, 0.26125138998031616, 0.32679590582847595, 0.3144976496696472, 0.2796432673931122, 0.23931029438972473, 0.25764763355255127, 0.2827151119709015, 0.280958354473114, 0.2896670699119568, 0.285290002822876, 0.2590409517288208, 0.2881363332271576, 0.22557075321674347, 0.29124748706817627, 0.2644105553627014, 0.2729727625846863, 0.26420432329177856, 0.31297028064727783, 0.31666022539138794, 0.2689953148365021, 0.2702769637107849, 0.3078113794326782, 0.2513645589351654, 0.2918260097503662, 0.22903308272361755, 0.29282456636428833, 0.25307711958885193, 0.29448121786117554, 0.2776736319065094, 0.262844055891037, 0.25585058331489563, 0.2806183099746704, 0.2848609685897827, 0.28165143728256226, 0.2677784562110901, 0.26553648710250854, 0.25322872400283813, 0.29012152552604675, 0.2865090072154999, 0.2480999231338501, 0.2410069704055786, 0.24501433968544006, 0.26010578870773315, 0.27486807107925415, 0.275681734085083, 0.32691115140914917, 0.2395210564136505, 0.28115078806877136, 0.26902085542678833, 0.25610050559043884, 0.29844802618026733, 0.2920919358730316, 0.2781996726989746, 0.2595232129096985, 0.26295390725135803, 0.2590658664703369, 0.28858262300491333, 0.2514339089393616, 0.2856002748012543, 0.2776471674442291, 0.2349167913198471, 0.28795957565307617, 0.26666179299354553, 0.23050588369369507, 0.2788388133049011, 0.27160149812698364, 0.25698912143707275, 0.3127982020378113, 0.3066049814224243, 0.2725543975830078, 0.27408289909362793, 0.25021180510520935, 0.2814772129058838, 0.2768273651599884, 0.26781654357910156, 0.25341302156448364, 0.25681746006011963, 0.27664515376091003, 0.27293992042541504, 0.2271685004234314, 0.2789301872253418, 0.2710954546928406, 0.22388648986816406, 0.2558687627315521, 0.26510170102119446, 0.26645269989967346, 0.2884881794452667, 0.28511905670166016, 0.2786211371421814, 0.28993892669677734, 0.26868724822998047, 0.24742454290390015, 0.2812415659427643, 0.25146785378456116, 0.3027108609676361, 0.3003522753715515, 0.2848712205886841, 0.3028373718261719, 0.29741963744163513, 0.2727165222167969, 0.28645187616348267, 0.24132567644119263, 0.2590363621711731, 0.24892374873161316, 0.2584323287010193, 0.2884913980960846, 0.2674235999584198, 0.2723068594932556, 0.2752494812011719, 0.2308933436870575] [0.21934035420417786, 0.2383013367652893, 0.2957116961479187, 0.291486531496048, 0.22093498706817627, 0.2769428789615631, 0.26984381675720215, 0.2820811867713928, 0.26870620250701904, 0.2720358669757843, 0.2647724747657776, 0.24214020371437073, 0.2738458514213562, 0.26226696372032166, 0.2825830578804016, 0.2669575810432434, 0.3225295841693878, 0.2569194436073303, 0.2714052200317383, 0.28586751222610474, 0.2567668855190277, 0.31958699226379395, 0.2747098207473755, 0.26304829120635986, 0.2248489260673523, 0.26469457149505615, 0.2802559435367584, 0.23674410581588745, 0.27526143193244934, 0.225366473197937, 0.2544100284576416, 0.25161316990852356, 0.2710040211677551, 0.2996249794960022, 0.2633834481239319, 0.2652689814567566, 0.27142083644866943, 0.2558267414569855, 0.2615845799446106, 0.24705791473388672, 0.251615434885025, 0.2824786603450775, 0.25308430194854736, 0.2728312611579895, 0.32364583015441895, 0.2781849503517151, 0.29394713044166565, 0.29475852847099304, 0.24971340596675873, 0.2660805583000183, 0.2718391716480255, 0.28256744146347046, 0.2561779022216797, 0.26050519943237305, 0.2644112706184387, 0.2530888319015503, 0.2580123543739319, 0.26835253834724426, 0.25192779302597046, 0.24943509697914124, 0.2800602912902832, 0.3178970515727997, 0.25434204936027527, 0.2541300058364868, 0.24799597263336182, 0.24609753489494324, 0.28156575560569763, 0.2830483615398407, 0.2591356039047241, 0.2728109359741211, 0.27360284328460693, 0.26397964358329773, 0.2780950665473938, 0.25785332918167114, 0.2904093563556671, 0.25567299127578735, 0.2727336287498474, 0.31347930431365967, 0.2708207964897156, 0.27445200085639954, 0.2706521451473236, 0.2778961956501007, 0.23875559866428375, 0.25145119428634644, 0.2482120245695114, 0.2753240466117859, 0.23467738926410675, 0.29527518153190613, 0.2519533336162567, 0.2718166708946228, 0.27358895540237427, 0.2571011781692505, 0.27170687913894653, 0.24680489301681519, 0.2745670974254608, 0.2525949478149414, 0.25169098377227783, 0.23517630994319916, 0.2570498585700989, 0.23877689242362976, 0.30685439705848694, 0.24027284979820251, 0.2518918812274933, 0.24114608764648438, 0.24905139207839966, 0.25137901306152344, 0.22464993596076965, 0.2793538570404053, 0.2665829658508301, 0.28465214371681213, 0.2751276195049286, 0.26439768075942993, 0.25297921895980835, 0.26010581851005554, 0.2290116548538208, 0.24685722589492798, 0.25731897354125977, 0.2780766487121582, 0.23677590489387512, 0.26401495933532715, 0.26425737142562866, 0.2548966109752655, 0.23475435376167297, 0.26540541648864746, 0.2288411259651184, 0.25697803497314453, 0.23689422011375427, 0.27508771419525146, 0.24609720706939697, 0.2657702565193176, 0.27651554346084595, 0.3058648109436035, 0.25669488310813904, 0.28941187262535095, 0.22257661819458008, 0.26888951659202576, 0.24106977880001068, 0.2596766948699951, 0.25544673204421997, 0.2718110978603363, 0.3113173842430115, 0.2297513484954834, 0.2631082832813263, 0.23325034976005554, 0.2832971215248108, 0.2723647952079773, 0.2305472046136856, 0.27956271171569824, 0.23771880567073822, 0.2835317850112915, 0.26439422369003296, 0.29074639081954956, 0.2469225525856018, 0.24780979752540588, 0.23024803400039673, 0.24553561210632324, 0.23489536345005035, 0.25422850251197815, 0.26333218812942505, 0.2613649368286133, 0.2726609706878662, 0.25331735610961914, 0.2736510932445526, 0.2588598132133484, 0.23339307308197021, 0.24783021211624146, 0.22405070066452026, 0.24722693860530853, 0.2556588649749756, 0.2664859890937805, 0.27526408433914185, 0.25171807408332825, 0.2383178174495697, 0.2930018901824951, 0.2561267018318176, 0.27823591232299805, 0.25664424896240234, 0.21307829022407532, 0.2670464515686035, 0.23410890996456146, 0.23334433138370514, 0.2703893482685089, 0.2536967396736145, 0.2870867848396301, 0.28825491666793823, 0.2504126727581024, 0.2640351355075836, 0.2923213243484497, 0.2695188522338867, 0.24995151162147522, 0.23527632653713226, 0.25744009017944336, 0.2675299644470215, 0.2428436279296875, 0.27075082063674927, 0.2593505382537842, 0.2785636782646179, 0.23280151188373566, 0.23846597969532013, 0.2796945571899414, 0.23787054419517517, 0.25391483306884766, 0.23699302971363068, 0.2831215560436249, 0.26190364360809326, 0.2683371603488922, 0.258228600025177, 0.23710395395755768, 0.2538378834724426, 0.22929714620113373, 0.245083749294281, 0.238290473818779, 0.25877827405929565, 0.30165591835975647, 0.26224297285079956, 0.24723681807518005, 0.26600977778434753, 0.2029835432767868, 0.260749876499176, 0.2194715440273285, 0.2390023022890091, 0.24525348842144012, 0.24073439836502075, 0.26444336771965027, 0.275911808013916, 0.2700945734977722, 0.27273231744766235, 0.2799711227416992, 0.249088317155838, 0.24108359217643738, 0.24693885445594788, 0.2638729214668274, 0.21866855025291443, 0.2495374083518982, 0.2925580143928528, 0.2363540530204773, 0.2571104168891907, 0.2288912534713745, 0.2502116858959198, 0.2697486877441406, 0.2509935796260834, 0.2500103712081909, 0.255179226398468, 0.2492743283510208, 0.24460023641586304, 0.22797638177871704, 0.24837227165699005, 0.2484590858221054, 0.259353369474411, 0.28913384675979614, 0.24896197021007538, 0.2304486334323883, 0.22529184818267822, 0.267991840839386, 0.2479018121957779, 0.21572084724903107, 0.22551530599594116, 0.24244605004787445, 0.24636563658714294, 0.23245516419410706, 0.2758826017379761, 0.2011304497718811, 0.25314173102378845, 0.22515232861042023, 0.2325555980205536, 0.24219433963298798, 0.23343920707702637, 0.23452219367027283, 0.25551021099090576, 0.24495568871498108, 0.2375691831111908, 0.2652064859867096, 0.2670786380767822, 0.2660090923309326, 0.22038865089416504, 0.24048656225204468, 0.22556909918785095, 0.2616906762123108, 0.26523634791374207, 0.23584216833114624, 0.2524954378604889, 0.25645577907562256, 0.2733851969242096, 0.2428625375032425, 0.25675055384635925, 0.2614338994026184, 0.27701401710510254, 0.23308810591697693, 0.2502453923225403, 0.28412219882011414, 0.2643601894378662, 0.25048553943634033, 0.22873631119728088, 0.24692031741142273, 0.247547447681427, 0.24642643332481384, 0.27413010597229004, 0.245108962059021, 0.24459321796894073, 0.25299015641212463, 0.2659267783164978, 0.25470176339149475, 0.23012757301330566, 0.25018319487571716, 0.21698129177093506, 0.24915942549705505, 0.23694145679473877, 0.24785685539245605, 0.2683255672454834, 0.23781639337539673, 0.256874680519104, 0.26600730419158936, 0.23460443317890167, 0.20123469829559326, 0.23059511184692383, 0.28034764528274536, 0.23805314302444458, 0.2447790801525116, 0.2930222749710083, 0.2414480745792389, 0.290241539478302, 0.2854200005531311, 0.2572462260723114, 0.2471718192100525, 0.24493449926376343, 0.2718283534049988, 0.24018733203411102, 0.23691381514072418, 0.21443018317222595, 0.2671250104904175, 0.22167840600013733, 0.23054653406143188, 0.2659471333026886, 0.2179548144340515, 0.22625099122524261, 0.2515321373939514, 0.2349632978439331, 0.26242905855178833, 0.23898954689502716, 0.2364821434020996, 0.21123886108398438, 0.2576943039894104, 0.24566008150577545, 0.2314729392528534, 0.22438949346542358, 0.2618844509124756, 0.24307546019554138, 0.2535495162010193, 0.22211484611034393, 0.24027523398399353, 0.2285451591014862, 0.21965467929840088, 0.2507651150226593, 0.20241102576255798, 0.22440475225448608, 0.24402296543121338, 0.2686055600643158, 0.22423192858695984, 0.21978500485420227, 0.23945105075836182, 0.23975315690040588, 0.267961323261261, 0.21543753147125244, 0.23475751280784607, 0.2546851634979248, 0.23137982189655304, 0.281880259513855, 0.24607402086257935, 0.23855003714561462, 0.2549293339252472, 0.22564294934272766, 0.2552758455276489, 0.2584415674209595, 0.25404518842697144, 0.23189175128936768, 0.22553859651088715, 0.2626335918903351, 0.23904353380203247, 0.24252477288246155, 0.24195915460586548, 0.22758880257606506, 0.24935133755207062, 0.21341381967067719, 0.2521182894706726, 0.2430262565612793, 0.23826810717582703, 0.24850215017795563, 0.21395565569400787, 0.23651716113090515, 0.22150704264640808, 0.24952298402786255, 0.2288476824760437, 0.2632032036781311, 0.2191154658794403, 0.2777044177055359, 0.2312440723180771, 0.22011658549308777, 0.22548939287662506, 0.22861360013484955, 0.22569309175014496, 0.2494383156299591, 0.23792006075382233, 0.2530140280723572, 0.23501437902450562, 0.2510300278663635, 0.222153440117836, 0.24110843241214752, 0.23610621690750122, 0.24062275886535645, 0.2488509714603424, 0.21851839125156403, 0.23765501379966736, 0.2731580138206482, 0.2798318564891815, 0.2371731996536255, 0.22219052910804749, 0.23893500864505768, 0.2349414974451065, 0.2467235028743744, 0.21023985743522644, 0.2480505108833313, 0.2552150785923004, 0.2298555076122284, 0.21083298325538635, 0.2562495470046997, 0.23223330080509186, 0.23738741874694824, 0.2534371018409729, 0.23165829479694366, 0.26140767335891724, 0.2367178499698639, 0.21271270513534546, 0.2246263325214386, 0.25924813747406006, 0.24245110154151917, 0.2910086214542389, 0.23784829676151276, 0.2185741662979126, 0.24504590034484863, 0.23158489167690277, 0.2538556456565857, 0.23254625499248505, 0.2569478750228882, 0.23541803658008575, 0.2400469034910202, 0.23286332190036774, 0.24753092229366302, 0.2581148147583008, 0.22010597586631775, 0.22134236991405487, 0.2375262975692749, 0.24731403589248657, 0.27279171347618103, 0.2371959686279297, 0.2514069080352783, 0.27491724491119385, 0.22818830609321594, 0.26802927255630493, 0.21938171982765198, 0.24024663865566254, 0.2015804648399353, 0.24317042529582977, 0.21131139993667603, 0.2764441967010498, 0.2282884269952774, 0.21520374715328217, 0.25554585456848145, 0.237045556306839, 0.23072665929794312, 0.25328174233436584, 0.22669903934001923, 0.22420424222946167, 0.23244941234588623, 0.2545258402824402, 0.24049770832061768, 0.20860716700553894, 0.26607853174209595, 0.2353494018316269, 0.21968719363212585, 0.21129988133907318, 0.22651565074920654, 0.22199837863445282, 0.24749834835529327, 0.22581247985363007, 0.23834459483623505, 0.2624990940093994, 0.2695556879043579, 0.226847842335701, 0.2577340602874756, 0.24449791014194489, 0.2542564272880554, 0.2264004945755005, 0.24656842648983002, 0.2263127863407135, 0.23118159174919128, 0.2606712579727173, 0.221609428524971, 0.2579685151576996, 0.2596963346004486, 0.20467737317085266, 0.24731822311878204, 0.2896324098110199, 0.220448300242424, 0.23190602660179138, 0.21648579835891724, 0.213856041431427, 0.22064414620399475, 0.2429809272289276, 0.23608554899692535, 0.24001099169254303, 0.2432369440793991, 0.23242399096488953, 0.2403174638748169, 0.23458081483840942, 0.20968638360500336, 0.22650250792503357, 0.25177478790283203, 0.215097576379776, 0.260855495929718, 0.23087361454963684, 0.2475612312555313, 0.20873720943927765, 0.2356472611427307, 0.24944736063480377, 0.23766164481639862, 0.23487824201583862, 0.2584328353404999, 0.22947102785110474, 0.21790377795696259, 0.2561078667640686, 0.2416391372680664, 0.2253764271736145, 0.19922763109207153, 0.23926232755184174, 0.2201703041791916, 0.23987841606140137, 0.21758809685707092, 0.22395175695419312, 0.2525804936885834, 0.20249584317207336, 0.25872665643692017, 0.21603608131408691, 0.23919816315174103, 0.23652033507823944, 0.25561589002609253, 0.22924043238162994, 0.19088633358478546, 0.25729164481163025, 0.19361194968223572, 0.2248082458972931, 0.24009069800376892, 0.2256612777709961, 0.22580435872077942, 0.25206148624420166, 0.2467353492975235, 0.2200675904750824, 0.23124274611473083, 0.2527233064174652, 0.22079885005950928, 0.24531923234462738, 0.20826813578605652, 0.21302901208400726, 0.23449137806892395, 0.20487821102142334, 0.21019065380096436, 0.20677471160888672, 0.21967563033103943, 0.225026935338974, 0.2201642394065857, 0.26612892746925354, 0.24019396305084229, 0.2623040974140167, 0.2000521421432495, 0.21909505128860474, 0.22240054607391357, 0.24093644320964813, 0.20307302474975586, 0.22993706166744232, 0.21449297666549683, 0.20944833755493164, 0.2276010513305664, 0.25013431906700134, 0.23542632162570953, 0.20713698863983154, 0.25560954213142395, 0.2224809229373932, 0.2220015525817871, 0.2236354947090149, 0.2524193823337555, 0.24154800176620483, 0.22384244203567505, 0.2063957154750824, 0.24036119878292084, 0.2229461669921875, 0.2638753652572632, 0.2397584617137909, 0.20935213565826416, 0.216197669506073, 0.219093918800354, 0.20354272425174713, 0.22257943451404572, 0.20980097353458405, 0.21966853737831116, 0.22410157322883606, 0.23201139271259308, 0.21496209502220154, 0.20799753069877625, 0.22390586137771606, 0.24443596601486206, 0.22099745273590088, 0.26611360907554626, 0.22886070609092712, 0.21075114607810974, 0.223859041929245, 0.22358885407447815, 0.21125438809394836, 0.26778095960617065, 0.22307847440242767, 0.21246522665023804, 0.21217191219329834, 0.22722020745277405, 0.2227388620376587, 0.21128486096858978, 0.23750019073486328, 0.25349631905555725, 0.218406543135643, 0.23088908195495605, 0.1996607482433319, 0.2476741373538971, 0.2255924642086029, 0.22549666464328766, 0.2359374463558197, 0.22036999464035034, 0.211415097117424, 0.22210539877414703, 0.2467898726463318, 0.22407293319702148, 0.20972830057144165, 0.23381732404232025, 0.24898822605609894, 0.22891196608543396, 0.20016583800315857, 0.24876555800437927, 0.25406429171562195, 0.21293899416923523, 0.22483223676681519, 0.25092342495918274, 0.22571809589862823, 0.2466464638710022, 0.21290205419063568, 0.21537837386131287, 0.2456500381231308, 0.23603114485740662, 0.21863499283790588, 0.27758386731147766, 0.23206289112567902, 0.23198261857032776, 0.22028833627700806, 0.21837234497070312, 0.23041971027851105, 0.23061157763004303, 0.21894124150276184, 0.21645452082157135, 0.2343320995569229, 0.24370113015174866, 0.23737390339374542, 0.2335352897644043, 0.21999222040176392, 0.22247792780399323, 0.23845942318439484, 0.2104519009590149, 0.24201436340808868, 0.22468301653862, 0.24997363984584808, 0.2140910029411316, 0.22414761781692505, 0.25737911462783813, 0.23661431670188904, 0.21438179910182953, 0.22780174016952515, 0.2349759042263031, 0.22284433245658875, 0.2287687063217163, 0.21773022413253784, 0.20802180469036102, 0.2433987259864807, 0.2231331169605255, 0.21411725878715515, 0.2554848790168762, 0.19621360301971436, 0.22870899736881256, 0.20672789216041565, 0.22646856307983398, 0.23925983905792236, 0.22622568905353546, 0.21721549332141876, 0.250762403011322, 0.23250143229961395, 0.23739464581012726, 0.21808215975761414, 0.22425448894500732, 0.229074627161026, 0.23705443739891052, 0.21480043232440948, 0.21784138679504395, 0.21651087701320648, 0.23926129937171936, 0.21627375483512878, 0.2254086434841156, 0.20966562628746033, 0.24359659850597382, 0.21182626485824585, 0.21407648921012878, 0.22397282719612122, 0.18528980016708374, 0.18959417939186096, 0.2427377700805664, 0.22968995571136475, 0.2340976595878601, 0.19325533509254456, 0.23154470324516296, 0.22825993597507477, 0.2353595495223999, 0.2043822556734085, 0.25689697265625, 0.20479467511177063, 0.21199358999729156, 0.217789888381958, 0.18537823855876923, 0.2643260955810547, 0.24565139412879944, 0.21459290385246277, 0.1952233612537384, 0.22286835312843323, 0.2293643057346344, 0.2252497673034668, 0.19389748573303223, 0.23702958226203918, 0.25435835123062134, 0.20864233374595642, 0.2141464650630951, 0.24087706208229065, 0.18274134397506714, 0.2266935408115387, 0.20057077705860138, 0.21578377485275269, 0.23851682245731354, 0.2256285697221756, 0.21681779623031616, 0.22134552896022797, 0.1886698305606842, 0.19626764953136444, 0.2208487093448639, 0.2467052936553955, 0.22816866636276245, 0.23696193099021912, 0.21740230917930603, 0.22710378468036652, 0.24785323441028595, 0.1923535168170929, 0.21768498420715332, 0.22310084104537964, 0.2237333357334137, 0.24117009341716766, 0.24012994766235352, 0.22249534726142883, 0.21318410336971283, 0.26187747716903687, 0.2243383526802063, 0.23413199186325073, 0.20821678638458252, 0.21303203701972961, 0.2325621396303177, 0.20474211871623993, 0.20244935154914856, 0.23103579878807068, 0.21111227571964264, 0.2268577218055725, 0.21101351082324982, 0.22857552766799927, 0.22330494225025177, 0.2296251654624939, 0.22375833988189697, 0.19529522955417633, 0.2525845170021057, 0.2101316750049591, 0.2307904064655304, 0.2149531990289688, 0.24090996384620667, 0.2558410167694092, 0.23327313363552094, 0.2001730501651764, 0.19059959053993225, 0.22948619723320007, 0.22225572168827057, 0.21974751353263855, 0.22311463952064514, 0.20700880885124207, 0.23202002048492432, 0.20681244134902954, 0.1940850168466568, 0.20961901545524597, 0.21656183898448944, 0.22449323534965515, 0.22861891984939575, 0.2392483651638031, 0.21801747381687164, 0.22761774063110352, 0.24299129843711853, 0.23768481612205505, 0.2133118212223053, 0.22434669733047485, 0.22066405415534973, 0.20569926500320435, 0.2303512990474701, 0.22890958189964294, 0.20363686978816986, 0.2190450131893158, 0.20200055837631226, 0.23357261717319489, 0.23863328993320465, 0.2200135439634323, 0.2027844786643982, 0.23170386254787445, 0.22050002217292786, 0.20088374614715576, 0.21732789278030396, 0.21609139442443848, 0.21493786573410034, 0.20549198985099792, 0.26779183745384216, 0.2178003191947937, 0.2345767319202423, 0.2462529093027115, 0.22184982895851135, 0.23298947513103485, 0.2214955985546112, 0.21362847089767456, 0.2242516428232193, 0.19073349237442017, 0.21520160138607025, 0.17949526011943817, 0.2147364616394043, 0.21137821674346924, 0.19997291266918182, 0.23111149668693542, 0.2289232760667801, 0.1959162950515747, 0.24908447265625, 0.23033276200294495, 0.20027770102024078, 0.23390430212020874, 0.19191503524780273, 0.2228214144706726, 0.21306899189949036, 0.18852479755878448, 0.2166881114244461, 0.1951466202735901, 0.21975299715995789, 0.22000324726104736, 0.1872948706150055, 0.1815991997718811, 0.25591808557510376, 0.18111135065555573, 0.21757075190544128, 0.19678817689418793, 0.20092062652111053, 0.23289501667022705, 0.225312739610672, 0.23089399933815002, 0.2386113703250885, 0.19530871510505676, 0.2248794436454773, 0.21156452596187592, 0.1967068910598755, 0.22267168760299683, 0.17459547519683838, 0.21930953860282898, 0.20582985877990723, 0.22417384386062622, 0.2067459225654602, 0.2007283717393875, 0.21202205121517181, 0.19684740900993347, 0.2330632209777832, 0.25913500785827637, 0.1847222000360489, 0.22295528650283813, 0.22445079684257507, 0.20802439749240875, 0.21087050437927246, 0.22554337978363037, 0.21464265882968903, 0.25686588883399963, 0.20123890042304993, 0.2078554630279541, 0.21598801016807556, 0.20776990056037903, 0.21041396260261536, 0.21503353118896484, 0.204545795917511, 0.21616286039352417, 0.20080499351024628, 0.2514503598213196, 0.2039988934993744, 0.21199551224708557, 0.24223242700099945, 0.2040664255619049, 0.23218989372253418, 0.20355695486068726, 0.2136479616165161, 0.22069551050662994, 0.22112515568733215, 0.21813154220581055, 0.20088323950767517, 0.20936034619808197, 0.205356627702713, 0.21195636689662933, 0.20909281075000763, 0.2018975466489792, 0.22532394528388977, 0.23490530252456665, 0.22200602293014526, 0.23515504598617554, 0.24595853686332703, 0.2284020334482193, 0.2511956989765167, 0.22444994747638702, 0.19349801540374756, 0.2377033829689026, 0.201593279838562, 0.2351032793521881, 0.22354328632354736, 0.2116045355796814, 0.18710187077522278, 0.22248873114585876, 0.20101842284202576, 0.21366068720817566] [0.18778346478939056, 0.24209651350975037, 0.2324765920639038, 0.21692240238189697, 0.19716480374336243, 0.18255028128623962, 0.21995064616203308, 0.2291533648967743, 0.21177011728286743, 0.23147258162498474, 0.2137104868888855, 0.23836301267147064, 0.21581047773361206, 0.19636785984039307, 0.19975309073925018, 0.20504596829414368, 0.21027228236198425, 0.20605096220970154, 0.20604214072227478, 0.19769564270973206, 0.2166084349155426, 0.2398093044757843, 0.2340078353881836, 0.2109798640012741, 0.20133429765701294, 0.2613474726676941, 0.23910579085350037, 0.20361778140068054, 0.229285329580307, 0.21822085976600647, 0.21129924058914185, 0.19655665755271912, 0.2109578251838684, 0.2129967212677002, 0.20380941033363342, 0.2012331336736679, 0.22626282274723053, 0.19279882311820984, 0.22038833796977997, 0.23080101609230042, 0.19449535012245178, 0.18790088593959808, 0.20997437834739685, 0.19413068890571594, 0.18628250062465668, 0.23514439165592194, 0.21967826783657074, 0.22163939476013184, 0.22962218523025513, 0.22419767081737518, 0.22342854738235474, 0.23351989686489105, 0.1913965344429016, 0.21219684183597565, 0.1728067547082901, 0.19562426209449768, 0.20645228028297424, 0.20777645707130432, 0.17935369908809662, 0.2010500282049179, 0.2146458625793457, 0.24014684557914734, 0.16279174387454987, 0.22188939154148102, 0.20205053687095642, 0.22177036106586456, 0.24158015847206116, 0.18630196154117584, 0.24601122736930847, 0.2254350781440735, 0.19900557398796082, 0.2064751535654068, 0.21302206814289093, 0.19474399089813232, 0.2062697559595108, 0.18844465911388397, 0.16865119338035583, 0.19524113833904266, 0.21566587686538696, 0.20220649242401123, 0.20448583364486694, 0.21080756187438965, 0.2166600376367569, 0.1900915950536728, 0.204171285033226, 0.1834130585193634, 0.19932925701141357, 0.21001462638378143, 0.22062014043331146, 0.2107543796300888, 0.2093789279460907, 0.20601458847522736, 0.23655198514461517, 0.23751382529735565, 0.21098488569259644, 0.2182053178548813, 0.17506611347198486, 0.20419499278068542, 0.1910475790500641, 0.1778048425912857, 0.24380101263523102, 0.2148289680480957, 0.19626158475875854, 0.1969814896583557, 0.21298876404762268, 0.20881271362304688, 0.20958268642425537, 0.22530829906463623, 0.19558902084827423, 0.21519549190998077, 0.1883036494255066, 0.190017968416214, 0.20167765021324158, 0.20735926926136017, 0.20487439632415771, 0.22684644162654877, 0.21196898818016052, 0.23017872869968414, 0.2252778857946396, 0.1964970827102661, 0.21212339401245117, 0.22445103526115417, 0.2105453461408615, 0.21652990579605103, 0.1964815855026245, 0.19218814373016357, 0.21960997581481934, 0.20616355538368225, 0.2176225781440735, 0.20790138840675354, 0.21643510460853577, 0.2257615327835083, 0.19901998341083527, 0.20319510996341705, 0.2114543318748474, 0.20802876353263855, 0.18677961826324463, 0.21453091502189636, 0.2260008603334427, 0.2234388291835785, 0.19057190418243408, 0.21074813604354858, 0.21499577164649963, 0.21791012585163116, 0.2166573852300644, 0.22919365763664246, 0.21020063757896423, 0.20840312540531158, 0.18495601415634155, 0.20662376284599304, 0.2276628166437149, 0.20610667765140533, 0.22294020652770996, 0.20434120297431946, 0.215559720993042, 0.2114630937576294, 0.2317965179681778, 0.1852913200855255, 0.2132072150707245, 0.19080789387226105, 0.1990237832069397, 0.19957268238067627, 0.18137474358081818, 0.20280703902244568, 0.20237082242965698, 0.20076605677604675, 0.20011092722415924, 0.19514867663383484, 0.19834493100643158, 0.2109876424074173, 0.1933130919933319, 0.21527567505836487, 0.19945144653320312, 0.24091866612434387, 0.21949529647827148, 0.1935732662677765, 0.2245854288339615, 0.21795040369033813, 0.19720809161663055, 0.2014043927192688, 0.23129907250404358, 0.20106801390647888, 0.21922168135643005, 0.20602881908416748, 0.22322309017181396, 0.24552693963050842, 0.2081301361322403, 0.22305968403816223, 0.20356020331382751, 0.20109498500823975, 0.19480803608894348, 0.1993049681186676, 0.20060983300209045, 0.1804133951663971, 0.22702932357788086, 0.22842732071876526, 0.2096475213766098, 0.23759284615516663, 0.20660501718521118, 0.19249281287193298, 0.23052513599395752, 0.22667527198791504, 0.24129930138587952, 0.19664129614830017, 0.19874617457389832, 0.18825805187225342, 0.1807989478111267, 0.2042805254459381, 0.19328156113624573, 0.21944484114646912, 0.22263023257255554, 0.17619167268276215, 0.1992206871509552, 0.20295417308807373, 0.19292424619197845, 0.21403855085372925, 0.21404008567333221, 0.19918444752693176, 0.22389106452465057, 0.19671232998371124, 0.21075186133384705, 0.19417604804039001, 0.20664173364639282, 0.18987151980400085, 0.19746534526348114, 0.23176950216293335, 0.21451207995414734, 0.1677904725074768, 0.21137382090091705, 0.22312164306640625, 0.20666681230068207, 0.22649599611759186, 0.2073569893836975, 0.18132974207401276, 0.21473023295402527, 0.21455281972885132, 0.1893320083618164, 0.2478332817554474, 0.20120243728160858, 0.19394421577453613, 0.20490266382694244, 0.24311956763267517, 0.20649072527885437, 0.18692979216575623, 0.21024708449840546, 0.22240906953811646, 0.21260055899620056, 0.18302111327648163, 0.22120700776576996, 0.20871952176094055, 0.1999346911907196, 0.2266661524772644, 0.20993489027023315, 0.19768063724040985, 0.19377601146697998, 0.19092702865600586, 0.23490563035011292, 0.19870051741600037, 0.19696113467216492, 0.21391671895980835, 0.19998541474342346, 0.2216072678565979, 0.18196596205234528, 0.1928226351737976, 0.17706580460071564, 0.18939660489559174, 0.18508394062519073, 0.20981265604496002, 0.21116860210895538, 0.1943289190530777, 0.19954772293567657, 0.19669324159622192, 0.17247161269187927, 0.19255296885967255, 0.20336522161960602, 0.21534240245819092, 0.20007823407649994, 0.19235515594482422, 0.20366330444812775, 0.20987451076507568, 0.2055875062942505, 0.2211623340845108, 0.21478277444839478, 0.19134831428527832, 0.202826589345932, 0.21497641503810883, 0.20012664794921875, 0.1790952980518341, 0.19327425956726074, 0.18771708011627197, 0.21740248799324036, 0.224236398935318, 0.17630308866500854, 0.216408833861351, 0.19522380828857422, 0.18037131428718567, 0.25853419303894043, 0.1757902204990387, 0.21857406198978424, 0.2442464530467987, 0.19644467532634735, 0.20273922383785248, 0.20634865760803223, 0.18599127233028412, 0.16416874527931213, 0.17185917496681213, 0.19039466977119446, 0.2124088704586029, 0.17954444885253906, 0.2093333601951599, 0.18183395266532898, 0.17262044548988342, 0.19081248342990875, 0.2022564560174942, 0.22275474667549133, 0.19425618648529053, 0.2383805811405182, 0.2064565122127533, 0.20674949884414673, 0.19022837281227112, 0.20418232679367065, 0.19825395941734314, 0.20557738840579987, 0.17159049212932587, 0.20647460222244263, 0.23685495555400848, 0.18546821177005768, 0.16878533363342285, 0.2010457068681717, 0.2030264139175415, 0.21156388521194458, 0.23805572092533112, 0.20222346484661102, 0.2635931372642517, 0.1947779506444931, 0.1956407129764557, 0.20488175749778748, 0.2082994282245636, 0.22636112570762634, 0.21460747718811035, 0.20186331868171692, 0.1872178316116333, 0.20041576027870178, 0.20071420073509216, 0.17717573046684265, 0.19508308172225952, 0.21727511286735535, 0.1903347671031952, 0.21413084864616394, 0.1857680380344391, 0.19319665431976318, 0.19678279757499695, 0.19360414147377014, 0.22506272792816162, 0.21324680745601654, 0.2256641387939453, 0.19442108273506165, 0.1924925297498703, 0.2087538242340088, 0.20717529952526093, 0.18746456503868103, 0.1816214621067047, 0.2175614833831787, 0.1962568312883377, 0.19264429807662964, 0.16496042907238007, 0.20086760818958282, 0.20652636885643005, 0.2097562551498413, 0.20708131790161133, 0.1990310549736023, 0.19027134776115417, 0.22227878868579865, 0.2090732306241989, 0.21009573340415955, 0.19360113143920898, 0.2014024257659912, 0.21497686207294464, 0.2171977311372757, 0.19706249237060547, 0.19567689299583435, 0.17808449268341064, 0.19883039593696594, 0.19746744632720947, 0.17829529941082, 0.1908063292503357, 0.19285930693149567, 0.20631450414657593, 0.19987566769123077, 0.22266098856925964, 0.18101510405540466, 0.19381183385849, 0.20619787275791168, 0.18620938062667847, 0.18873265385627747, 0.20576506853103638, 0.20937052369117737, 0.19048447906970978, 0.18701007962226868, 0.20935308933258057, 0.18725784122943878, 0.19871090352535248, 0.1834009289741516, 0.16941165924072266, 0.1913948655128479, 0.22229328751564026, 0.1855797916650772, 0.1789446622133255, 0.21881040930747986, 0.18275713920593262, 0.19085533916950226, 0.2114683985710144, 0.17337961494922638, 0.18786078691482544, 0.1980324387550354, 0.19415166974067688, 0.21780496835708618, 0.20013338327407837, 0.19710539281368256, 0.19501008093357086, 0.18113698065280914, 0.2094660848379135, 0.21586453914642334, 0.19663478434085846, 0.16642692685127258, 0.1986284703016281, 0.1886684000492096, 0.22005122900009155, 0.21646521985530853, 0.1785506308078766, 0.17309680581092834, 0.19221638143062592, 0.1908043622970581, 0.21368363499641418, 0.21235860884189606, 0.21590834856033325, 0.19785350561141968, 0.1778591424226761, 0.2043403685092926, 0.20791712403297424, 0.21247731149196625, 0.18218228220939636, 0.2047678530216217, 0.19793501496315002, 0.20192965865135193, 0.1997530460357666, 0.1949441134929657, 0.20150934159755707, 0.19970378279685974, 0.21564777195453644, 0.18877005577087402, 0.20857127010822296, 0.16987718641757965, 0.2004753202199936, 0.19242939352989197, 0.18334944546222687, 0.21160249412059784, 0.1627034693956375, 0.17256538569927216, 0.16489726305007935, 0.20320436358451843, 0.16726046800613403, 0.2068859040737152, 0.19921696186065674, 0.179029643535614, 0.18468588590621948, 0.18324372172355652, 0.186465322971344, 0.19668170809745789, 0.19652491807937622, 0.1804211586713791, 0.17848055064678192, 0.17519882321357727, 0.20154860615730286, 0.1858413815498352, 0.20983199775218964, 0.1995326578617096, 0.17950084805488586, 0.19526946544647217, 0.19323542714118958, 0.19850072264671326, 0.19849443435668945, 0.23018880188465118, 0.20032694935798645, 0.19471773505210876, 0.19464369118213654, 0.2333347499370575, 0.20446264743804932, 0.19887295365333557, 0.18827709555625916, 0.18788191676139832, 0.2134959101676941, 0.19852140545845032, 0.17664313316345215, 0.17788243293762207, 0.19285371899604797, 0.1781635582447052, 0.18233075737953186, 0.20687061548233032, 0.20027664303779602, 0.17395856976509094, 0.17574892938137054, 0.1716364175081253, 0.18040770292282104, 0.1937330663204193, 0.19236642122268677, 0.17691266536712646, 0.19779512286186218, 0.19108301401138306, 0.17133279144763947, 0.17965200543403625, 0.20013180375099182, 0.1904641091823578, 0.2030051052570343, 0.1870097517967224, 0.1931639462709427, 0.20209690928459167, 0.19016066193580627, 0.19354182481765747, 0.21408683061599731, 0.17387044429779053, 0.1948968768119812, 0.2090642750263214, 0.15868830680847168, 0.2277170568704605, 0.18384088575839996, 0.19396398961544037, 0.17675504088401794, 0.19612935185432434, 0.21523833274841309, 0.19324952363967896, 0.2218412160873413, 0.19434133172035217, 0.18677359819412231, 0.1855727732181549, 0.19304513931274414, 0.18632084131240845, 0.20899567008018494, 0.18776234984397888, 0.18870271742343903, 0.19038507342338562, 0.2173682153224945, 0.20289349555969238, 0.18497294187545776, 0.20221002399921417, 0.17878153920173645, 0.18645353615283966, 0.17342323064804077, 0.18571719527244568, 0.22340279817581177, 0.18067710101604462, 0.21291232109069824, 0.17716291546821594, 0.20081812143325806, 0.19501377642154694, 0.18703781068325043, 0.17884990572929382, 0.2153264880180359, 0.15975385904312134, 0.18202191591262817, 0.20342591404914856, 0.16241039335727692, 0.21595340967178345, 0.16939091682434082, 0.19932471215724945, 0.1685638576745987, 0.16283047199249268, 0.19280561804771423, 0.21782708168029785, 0.15751349925994873, 0.19117890298366547, 0.18237704038619995, 0.1803305745124817, 0.1918283998966217, 0.18482255935668945, 0.1694253832101822, 0.18493756651878357, 0.20744863152503967, 0.20765116810798645, 0.19894245266914368, 0.2223881036043167, 0.19156111776828766, 0.18549659848213196, 0.20191887021064758, 0.17907850444316864, 0.1962367296218872, 0.193038672208786, 0.18206781148910522, 0.18492533266544342, 0.17247071862220764, 0.17584151029586792, 0.2071598470211029, 0.2187730073928833, 0.17182987928390503, 0.17329633235931396, 0.203128844499588, 0.19309121370315552, 0.18618306517601013, 0.1866840124130249, 0.1754177063703537, 0.19778814911842346, 0.1707877218723297, 0.18385134637355804, 0.18031543493270874, 0.18867073953151703, 0.201484814286232, 0.16815979778766632, 0.21397359669208527, 0.18112576007843018, 0.19793987274169922, 0.23962078988552094, 0.18526144325733185, 0.1863887459039688, 0.19386744499206543, 0.19587087631225586, 0.19431214034557343, 0.18526455760002136, 0.19125621020793915, 0.17149637639522552, 0.18563337624073029, 0.20845556259155273, 0.1837673783302307, 0.1958787590265274, 0.20682042837142944, 0.1763409525156021, 0.18961110711097717, 0.17455895245075226, 0.18806611001491547, 0.18480050563812256, 0.2000253051519394, 0.18332991003990173, 0.17517085373401642, 0.19696776568889618, 0.19183649122714996, 0.1542377471923828, 0.191900372505188, 0.19730547070503235, 0.21749535202980042, 0.16062839329242706, 0.190250962972641, 0.19203634560108185, 0.2081344723701477, 0.18582221865653992, 0.21929588913917542, 0.2004251778125763, 0.183821901679039, 0.18547388911247253, 0.17516416311264038, 0.18919691443443298, 0.1602640151977539, 0.20858071744441986, 0.19242212176322937, 0.17546240985393524, 0.1642232984304428, 0.20160657167434692, 0.186383917927742, 0.19248004257678986, 0.19161948561668396, 0.18814221024513245, 0.20450632274150848, 0.15946072340011597, 0.17400074005126953, 0.1938985288143158, 0.19349205493927002, 0.1831343173980713, 0.19987303018569946, 0.23737721145153046, 0.1738840937614441, 0.18685302138328552, 0.1892244815826416, 0.17462247610092163, 0.1971132755279541, 0.1777423769235611, 0.19237466156482697, 0.1842374950647354, 0.21210482716560364, 0.1904354989528656, 0.19372105598449707, 0.1880585253238678, 0.1723528802394867, 0.20670610666275024, 0.18271544575691223, 0.22470217943191528, 0.19084593653678894, 0.2205020636320114, 0.19607827067375183, 0.19910834729671478, 0.1663713902235031, 0.1839364618062973, 0.169864684343338, 0.17505064606666565, 0.1698581874370575, 0.18030954897403717, 0.190797358751297, 0.17383074760437012, 0.20625877380371094, 0.19515174627304077, 0.20414158701896667, 0.20639720559120178, 0.1997535228729248, 0.20908284187316895, 0.1930883228778839, 0.18048688769340515, 0.17529037594795227, 0.190755695104599, 0.17156532406806946, 0.18035097420215607, 0.18635863065719604, 0.1728944480419159, 0.17821601033210754, 0.17924226820468903, 0.19049151241779327, 0.1811729073524475, 0.21724095940589905, 0.19081388413906097, 0.18190929293632507, 0.15475702285766602, 0.17064064741134644, 0.1928224116563797, 0.16841481626033783, 0.18949167430400848, 0.18646037578582764, 0.17366912961006165, 0.19149169325828552, 0.1920803189277649, 0.18804576992988586, 0.2208564728498459, 0.20612463355064392, 0.17610406875610352, 0.19426217675209045, 0.2185688316822052, 0.1892511248588562, 0.18442462384700775, 0.18416333198547363, 0.1856372207403183, 0.16272073984146118, 0.18916338682174683, 0.1894356608390808, 0.1879662275314331, 0.18989896774291992, 0.18644869327545166, 0.18864546716213226, 0.1993149369955063, 0.18515820801258087, 0.19810351729393005, 0.22263997793197632, 0.20967715978622437, 0.2205876111984253, 0.18452000617980957, 0.19218167662620544, 0.19067345559597015, 0.1861245483160019, 0.17583909630775452, 0.17667913436889648, 0.16728124022483826, 0.17906805872917175, 0.17703688144683838, 0.18328934907913208, 0.17025351524353027, 0.19057157635688782, 0.18127596378326416, 0.1539996862411499, 0.16418063640594482, 0.20235757529735565, 0.18032322824001312, 0.1837765872478485, 0.16323497891426086, 0.20086628198623657, 0.18147805333137512, 0.17061088979244232, 0.16178810596466064, 0.16414213180541992, 0.1829441636800766, 0.21961353719234467, 0.20080918073654175, 0.15202225744724274, 0.17846979200839996, 0.1801067441701889, 0.1884174644947052, 0.2248183786869049, 0.19538617134094238, 0.19190621376037598, 0.19517916440963745, 0.15376335382461548, 0.19453391432762146, 0.2102433741092682, 0.144766703248024, 0.18017618358135223, 0.1789085566997528, 0.18910588324069977, 0.1754489243030548, 0.18955105543136597, 0.16671320796012878, 0.18647921085357666, 0.1838347166776657, 0.17737990617752075, 0.18903498351573944, 0.17348775267601013, 0.18827907741069794, 0.155684694647789, 0.19040778279304504, 0.1910782903432846, 0.18646079301834106, 0.17877447605133057, 0.18903851509094238, 0.17771531641483307, 0.17947587370872498, 0.1693500578403473, 0.19210925698280334, 0.17862391471862793, 0.1702100783586502, 0.17764078080654144, 0.17116570472717285, 0.172437846660614, 0.18073734641075134, 0.20251202583312988, 0.19208654761314392, 0.18463964760303497, 0.19636186957359314, 0.18527641892433167, 0.17825180292129517, 0.16876420378684998, 0.1951073408126831, 0.1932123601436615, 0.197988361120224, 0.18685342371463776, 0.17208662629127502, 0.18239732086658478, 0.1819276064634323, 0.16747823357582092, 0.18252220749855042, 0.1940375715494156, 0.18901322782039642, 0.1736757457256317, 0.16104474663734436, 0.20110464096069336, 0.17219296097755432, 0.19504433870315552, 0.1775449812412262, 0.20510771870613098, 0.17131713032722473, 0.18118712306022644, 0.16891631484031677, 0.15641793608665466, 0.17270144820213318, 0.16942360997200012, 0.18608662486076355, 0.18761689960956573, 0.18124321103096008, 0.1900818645954132, 0.18233251571655273, 0.18066900968551636, 0.17758691310882568, 0.1656859964132309, 0.20312492549419403, 0.16326400637626648, 0.18433445692062378, 0.18332237005233765, 0.18388573825359344, 0.18196624517440796, 0.185797780752182, 0.18400466442108154, 0.1936207413673401, 0.18513423204421997, 0.1678832471370697, 0.173165425658226, 0.1707230806350708, 0.19067619740962982, 0.19039443135261536, 0.16417540609836578, 0.15202461183071136, 0.1852494776248932, 0.18180438876152039, 0.1834108531475067, 0.18981662392616272, 0.16906605660915375, 0.16121578216552734, 0.19638538360595703, 0.17836834490299225, 0.1741398274898529, 0.17143414914608002, 0.16856247186660767, 0.16136932373046875, 0.18259930610656738, 0.15755581855773926, 0.2034948617219925, 0.17929287254810333, 0.1721464991569519, 0.18277814984321594, 0.1973211020231247, 0.15917328000068665, 0.1720232516527176, 0.18150931596755981, 0.17672483623027802, 0.18728330731391907, 0.1934138834476471, 0.1722344011068344, 0.19047579169273376, 0.1699039340019226, 0.20399731397628784, 0.15943141281604767, 0.19373685121536255, 0.17945526540279388, 0.20432761311531067, 0.18435141444206238, 0.1928497552871704, 0.19456741213798523, 0.1909179538488388, 0.16209040582180023, 0.19330820441246033, 0.17606037855148315, 0.19644400477409363, 0.18255871534347534, 0.19072334468364716, 0.1933252364397049, 0.17342467606067657, 0.16094300150871277, 0.1966439187526703, 0.154901921749115, 0.18520402908325195, 0.18833637237548828, 0.18420472741127014, 0.17133945226669312, 0.17681415379047394, 0.17843151092529297, 0.1828920692205429, 0.1981048285961151, 0.181965172290802, 0.13798224925994873, 0.174066424369812, 0.16754290461540222, 0.16645586490631104, 0.16406792402267456, 0.1937991827726364, 0.17433542013168335, 0.1681043952703476, 0.16336867213249207, 0.1728372871875763] [0.18815580010414124, 0.17633464932441711, 0.20623749494552612, 0.20468471944332123, 0.19635644555091858, 0.17947836220264435, 0.18503093719482422, 0.147699773311615, 0.1941717565059662, 0.183034285902977, 0.18301737308502197, 0.193662628531456, 0.17565330862998962, 0.16002865135669708, 0.16078075766563416, 0.1648348867893219, 0.19903714954853058, 0.19377990067005157, 0.17643311619758606, 0.17352306842803955, 0.18457350134849548, 0.18805064260959625, 0.17070609331130981, 0.17809510231018066, 0.16372838616371155, 0.17564217746257782, 0.16061095893383026, 0.16424202919006348, 0.17546087503433228, 0.1663990318775177, 0.16720515489578247, 0.20053310692310333, 0.17064188420772552, 0.17955376207828522, 0.18100836873054504, 0.16923606395721436, 0.17977307736873627, 0.16788873076438904, 0.21038353443145752, 0.1764356791973114, 0.17613205313682556, 0.16884209215641022, 0.17372438311576843, 0.14281204342842102, 0.18611223995685577, 0.19538936018943787, 0.1632995307445526, 0.15661442279815674, 0.14559419453144073, 0.18632161617279053, 0.19717155396938324, 0.19195404648780823, 0.19041547179222107, 0.1856568455696106, 0.1707969307899475, 0.176004558801651, 0.16034244000911713, 0.18691450357437134, 0.15866242349147797, 0.1880534291267395, 0.1674930453300476, 0.17879736423492432, 0.1905026137828827, 0.15920275449752808, 0.1732766330242157, 0.17001821100711823, 0.1667827069759369, 0.18206430971622467, 0.16588646173477173, 0.1675010323524475, 0.177590012550354, 0.1844945102930069, 0.17318931221961975, 0.16721391677856445, 0.21298947930335999, 0.17175237834453583, 0.1522878259420395, 0.1788097620010376, 0.1719854772090912, 0.18455247581005096, 0.17348206043243408, 0.16462382674217224, 0.1810240000486374, 0.17200849950313568, 0.16570402681827545, 0.19941823184490204, 0.1730688214302063, 0.18163132667541504, 0.1920461654663086, 0.17064327001571655, 0.1995857059955597, 0.17967721819877625, 0.17212487757205963, 0.19901707768440247, 0.1905936300754547, 0.16888120770454407, 0.18502524495124817, 0.1792050153017044, 0.1813172698020935, 0.21015188097953796, 0.1888006329536438, 0.16187116503715515, 0.14844654500484467, 0.17814962565898895, 0.17212440073490143, 0.1765647530555725, 0.17193162441253662, 0.1610134094953537, 0.17804309725761414, 0.1808442771434784, 0.15241724252700806, 0.18350283801555634, 0.18420326709747314, 0.18430164456367493, 0.18133366107940674, 0.17639529705047607, 0.16930094361305237, 0.15233036875724792, 0.17618674039840698, 0.15106861293315887, 0.21795213222503662, 0.1589704006910324, 0.17337077856063843, 0.1548561155796051, 0.17484325170516968, 0.17301952838897705, 0.19809800386428833, 0.2024421989917755, 0.16267681121826172, 0.16681745648384094, 0.18341735005378723, 0.17817915976047516, 0.18702585995197296, 0.18526603281497955, 0.1739000827074051, 0.1862727403640747, 0.18011024594306946, 0.18714720010757446, 0.1649669110774994, 0.21523717045783997, 0.19021451473236084, 0.1807359904050827, 0.1794850081205368, 0.16590337455272675, 0.18895891308784485, 0.19827060401439667, 0.12863650918006897, 0.17993272840976715, 0.1645965278148651, 0.17973750829696655, 0.15761509537696838, 0.209843248128891, 0.16612914204597473, 0.18953034281730652, 0.1912483125925064, 0.15817958116531372, 0.17330843210220337, 0.1574746072292328, 0.1609361171722412, 0.16355520486831665, 0.19972114264965057, 0.19048577547073364, 0.19306764006614685, 0.16046801209449768, 0.15087956190109253, 0.16092932224273682, 0.17611002922058105, 0.1633988618850708, 0.1764780879020691, 0.17928074300289154, 0.16165734827518463, 0.16433387994766235, 0.17301759123802185, 0.1932888776063919, 0.18997517228126526, 0.17592419683933258, 0.169883593916893, 0.19661816954612732, 0.19198565185070038, 0.1540936827659607, 0.2036769837141037, 0.1818949431180954, 0.1609816998243332, 0.18959832191467285, 0.1791713535785675, 0.1567155122756958, 0.17530861496925354, 0.1682288646697998, 0.17227911949157715, 0.20352578163146973, 0.18436577916145325, 0.17989176511764526, 0.1762956976890564, 0.17681758105754852, 0.16188672184944153, 0.15464144945144653, 0.18304720520973206, 0.16779187321662903, 0.15393701195716858, 0.157396137714386, 0.18456950783729553, 0.16487914323806763, 0.1660608947277069, 0.18064261972904205, 0.18002475798130035, 0.17768719792366028, 0.1778089702129364, 0.16678744554519653, 0.18582147359848022, 0.16777800023555756, 0.15079355239868164, 0.17960511147975922, 0.16276688873767853, 0.17131109535694122, 0.17783254384994507, 0.1690867841243744, 0.17672425508499146, 0.16286203265190125, 0.17350783944129944, 0.18259674310684204, 0.17193838953971863, 0.18992160260677338, 0.16375119984149933, 0.16715094447135925, 0.169089674949646, 0.18030373752117157, 0.16005992889404297, 0.18793557584285736, 0.19002430140972137, 0.16674470901489258, 0.18213409185409546, 0.1904827356338501, 0.14935429394245148, 0.19069860875606537, 0.16069093346595764, 0.19250009953975677, 0.18505576252937317, 0.16699360311031342, 0.16626383364200592, 0.18680952489376068, 0.17059193551540375, 0.16392485797405243, 0.17787516117095947, 0.17088386416435242, 0.19135808944702148, 0.17196407914161682, 0.17956890165805817, 0.17148259282112122, 0.1810913234949112, 0.16832861304283142, 0.18673637509346008, 0.15612496435642242, 0.18372789025306702, 0.1496325135231018, 0.18177708983421326, 0.16578003764152527, 0.16996322572231293, 0.1657661646604538, 0.17742636799812317, 0.17482246458530426, 0.18020077049732208, 0.17960625886917114, 0.14990359544754028, 0.162187397480011, 0.14594867825508118, 0.1690373718738556, 0.16158226132392883, 0.1510675698518753, 0.17336004972457886, 0.16327306628227234, 0.190937340259552, 0.17587770521640778, 0.17095883190631866, 0.1865161955356598, 0.1904122680425644, 0.15893450379371643, 0.1849476844072342, 0.16579669713974, 0.18353235721588135, 0.17617586255073547, 0.1578943133354187, 0.15263155102729797, 0.16135285794734955, 0.1731187105178833, 0.18608948588371277, 0.17954054474830627, 0.16861499845981598, 0.17815513908863068, 0.18054088950157166, 0.16382057964801788, 0.163371741771698, 0.14582273364067078, 0.15513038635253906, 0.1540525257587433, 0.15559715032577515, 0.16321837902069092, 0.18536779284477234, 0.18987920880317688, 0.16334345936775208, 0.18714812397956848, 0.15976928174495697, 0.191618412733078, 0.16846975684165955, 0.17681898176670074, 0.18397413194179535, 0.16262680292129517, 0.16553819179534912, 0.1675274819135666, 0.16486021876335144, 0.16173212230205536, 0.19501398503780365, 0.15166790783405304, 0.18384943902492523, 0.20117159187793732, 0.16799084842205048, 0.18967191874980927, 0.17178630828857422, 0.18107838928699493, 0.18627622723579407, 0.18915842473506927, 0.17059528827667236, 0.1498919427394867, 0.16964802145957947, 0.1621934175491333, 0.17407935857772827, 0.16165021061897278, 0.17129811644554138, 0.15432950854301453, 0.17968997359275818, 0.19601339101791382, 0.14622902870178223, 0.17821148037910461, 0.15705198049545288, 0.1671048253774643, 0.1656273603439331, 0.17183950543403625, 0.14914259314537048, 0.16063246130943298, 0.1387980878353119, 0.16964447498321533, 0.18132410943508148, 0.1749522089958191, 0.1547354757785797, 0.16537894308567047, 0.17951402068138123, 0.18677571415901184, 0.18778717517852783, 0.16450703144073486, 0.18755893409252167, 0.15610142052173615, 0.17252221703529358, 0.15999434888362885, 0.19594153761863708, 0.1699390411376953, 0.17469142377376556, 0.1573643684387207, 0.17605653405189514, 0.16528503596782684, 0.16595271229743958, 0.17490282654762268, 0.17295506596565247, 0.14412708580493927, 0.18318764865398407, 0.16877420246601105, 0.16014689207077026, 0.1728385090827942, 0.1741505116224289, 0.16249792277812958, 0.16925553977489471, 0.15472841262817383, 0.18300452828407288, 0.14294475317001343, 0.17457503080368042, 0.15862111747264862, 0.184637188911438, 0.1507892906665802, 0.17968414723873138, 0.17517685890197754, 0.16278353333473206, 0.1787400245666504, 0.16971541941165924, 0.16826187074184418, 0.1375221461057663, 0.1909503936767578, 0.1840977966785431, 0.16362223029136658, 0.14760257303714752, 0.16038987040519714, 0.16065236926078796, 0.17213937640190125, 0.1797182261943817, 0.1787179559469223, 0.15221962332725525, 0.1916711926460266, 0.17457139492034912, 0.17510254681110382, 0.15200597047805786, 0.17337632179260254, 0.17215996980667114, 0.1466594934463501, 0.15403985977172852, 0.16674642264842987, 0.18361347913742065, 0.17174369096755981, 0.14853595197200775, 0.17743077874183655, 0.15244677662849426, 0.1657879650592804, 0.19292721152305603, 0.20166684687137604, 0.18179117143154144, 0.17460252344608307, 0.16387386620044708, 0.1756870001554489, 0.160943403840065, 0.15628507733345032, 0.17547902464866638, 0.16078248620033264, 0.16609850525856018, 0.169780433177948, 0.1748068779706955, 0.1688791811466217, 0.18000754714012146, 0.18917043507099152, 0.1575372964143753, 0.1511518806219101, 0.18382593989372253, 0.16135966777801514, 0.14523029327392578, 0.1735893189907074, 0.17121988534927368, 0.1575644612312317, 0.16917967796325684, 0.14741671085357666, 0.1626754105091095, 0.17535185813903809, 0.16542932391166687, 0.1823212057352066, 0.16021369397640228, 0.20561692118644714, 0.18017080426216125, 0.17168402671813965, 0.13304342329502106, 0.16981855034828186, 0.15410052239894867, 0.17692846059799194, 0.1820419281721115, 0.15870144963264465, 0.16538359224796295, 0.15336942672729492, 0.1518143117427826, 0.14703471958637238, 0.1626896858215332, 0.162529855966568, 0.15952733159065247, 0.16342923045158386, 0.14184531569480896, 0.15434451401233673, 0.17640355229377747, 0.1653960645198822, 0.1588594764471054, 0.14580124616622925, 0.16741162538528442, 0.15230217576026917, 0.16334466636180878, 0.16078075766563416, 0.16064298152923584, 0.13757063448429108, 0.1985483467578888, 0.1722777783870697, 0.16768880188465118, 0.15644364058971405, 0.1675790548324585, 0.15315380692481995, 0.16654008626937866, 0.17900854349136353, 0.1645970344543457, 0.1696600466966629, 0.15100553631782532, 0.15469811856746674, 0.20043739676475525, 0.21169671416282654, 0.1811811476945877, 0.16989275813102722, 0.18747353553771973, 0.15596231818199158, 0.17520567774772644, 0.16929514706134796, 0.15775781869888306, 0.18837153911590576, 0.14310508966445923, 0.180518239736557, 0.14108170568943024, 0.1435149610042572, 0.1741320788860321, 0.18330198526382446, 0.1696905642747879, 0.17499424517154694, 0.16487732529640198, 0.17250168323516846, 0.1891036033630371, 0.1562797576189041, 0.1696300506591797, 0.1468173712491989, 0.16804538667201996, 0.14999864995479584, 0.16846759617328644, 0.1884021908044815, 0.1725689321756363, 0.16334843635559082, 0.14892274141311646, 0.15659038722515106, 0.17670002579689026, 0.14842453598976135, 0.16719140112400055, 0.16252289712429047, 0.15505021810531616, 0.17641131579875946, 0.15022626519203186, 0.159364253282547, 0.18258780241012573, 0.14610320329666138, 0.1591724455356598, 0.17241685092449188, 0.1799541711807251, 0.17520222067832947, 0.18764793872833252, 0.16973307728767395, 0.15883395075798035, 0.15597863495349884, 0.1586642861366272, 0.17046430706977844, 0.15530626475811005, 0.1408383846282959, 0.174726203083992, 0.1436341106891632, 0.15191979706287384, 0.15427306294441223, 0.14319713413715363, 0.16593044996261597, 0.1509198099374771, 0.17452453076839447, 0.193233460187912, 0.17005068063735962, 0.1565229594707489, 0.1708175241947174, 0.170274019241333, 0.17835262417793274, 0.15255135297775269, 0.1504901796579361, 0.1584048867225647, 0.1663537323474884, 0.16145703196525574, 0.15195828676223755, 0.16507330536842346, 0.14793692529201508, 0.15139929950237274, 0.19234979152679443, 0.1566278040409088, 0.17690715193748474, 0.18056386709213257, 0.14957383275032043, 0.15893642604351044, 0.16364739835262299, 0.1866355538368225, 0.172529399394989, 0.1690782606601715, 0.17073529958724976, 0.16695891320705414, 0.1746636927127838, 0.1643712818622589, 0.17100954055786133, 0.16117241978645325, 0.14925937354564667, 0.1465226411819458, 0.1693345457315445, 0.14618819952011108, 0.1654859334230423, 0.15601736307144165, 0.14735466241836548, 0.1688612699508667, 0.18755801022052765, 0.16908568143844604, 0.17548757791519165, 0.14193665981292725, 0.15470853447914124, 0.1616160124540329, 0.1554393768310547, 0.15978816151618958, 0.1703212708234787, 0.172834575176239, 0.165323868393898, 0.16935619711875916, 0.1721649467945099, 0.1730557680130005, 0.1509024202823639, 0.16453400254249573, 0.18350040912628174, 0.16398587822914124, 0.15918102860450745, 0.15938372910022736, 0.15573257207870483, 0.14721158146858215, 0.1624954640865326, 0.15799954533576965, 0.17029349505901337, 0.14319026470184326, 0.16705335676670074, 0.18790464103221893, 0.15941399335861206, 0.14845740795135498, 0.14806276559829712, 0.15784427523612976, 0.1607603281736374, 0.18479397892951965, 0.16000746190547943, 0.16732600331306458, 0.1831291913986206, 0.16904738545417786, 0.1498899757862091, 0.1740841269493103, 0.16933326423168182, 0.16420383751392365, 0.18307353556156158, 0.1795538365840912, 0.17606526613235474, 0.18607240915298462, 0.16038373112678528, 0.15530042350292206, 0.15776780247688293, 0.16733217239379883, 0.157143697142601, 0.15553948283195496, 0.16471369564533234, 0.1453436315059662, 0.16760683059692383, 0.14454501867294312, 0.15113824605941772, 0.162995845079422, 0.13432630896568298, 0.17327222228050232, 0.14901390671730042, 0.1423519253730774, 0.15681397914886475, 0.19182994961738586, 0.14727598428726196, 0.1698829084634781, 0.18542835116386414, 0.1475096493959427, 0.15155227482318878, 0.17094537615776062, 0.16959552466869354, 0.16935215890407562, 0.17965738475322723, 0.15821924805641174, 0.15718331933021545, 0.17536312341690063, 0.17422088980674744, 0.18877841532230377, 0.15153411030769348, 0.13958635926246643, 0.16175037622451782, 0.17036238312721252, 0.16797485947608948, 0.17747139930725098, 0.15149401128292084, 0.1728340983390808, 0.1767755150794983, 0.15610744059085846, 0.14892326295375824, 0.17945149540901184, 0.18025818467140198, 0.1498696506023407, 0.1486537754535675, 0.18413133919239044, 0.18135540187358856, 0.1481175422668457, 0.16727814078330994, 0.17789286375045776, 0.13974831998348236, 0.1781095713376999, 0.14408963918685913, 0.19498610496520996, 0.15409602224826813, 0.1793026328086853, 0.18273571133613586, 0.1653852015733719, 0.15972301363945007, 0.15296733379364014, 0.1377234011888504, 0.1663968861103058, 0.17167450487613678, 0.148562490940094, 0.18016870319843292, 0.20286747813224792, 0.15026801824569702, 0.17571373283863068, 0.1587473750114441, 0.17766252160072327, 0.1625203788280487, 0.17252251505851746, 0.16064628958702087, 0.17260120809078217, 0.15497741103172302, 0.1488248109817505, 0.15743716061115265, 0.15023648738861084, 0.17997685074806213, 0.16563035547733307, 0.17811548709869385, 0.14388135075569153, 0.1368785947561264, 0.1693733185529709, 0.14619432389736176, 0.1654457151889801, 0.14661507308483124, 0.15536396205425262, 0.1432548463344574, 0.1685899943113327, 0.16528069972991943, 0.16055837273597717, 0.17103484272956848, 0.1622369885444641, 0.13681983947753906, 0.16199588775634766, 0.15792003273963928, 0.1432994306087494, 0.14663445949554443, 0.15715014934539795, 0.1577734351158142, 0.1561017483472824, 0.1506316065788269, 0.1390649378299713, 0.17032268643379211, 0.1478433907032013, 0.15113970637321472, 0.1457970142364502, 0.16275274753570557, 0.1572571098804474, 0.18842755258083344, 0.13914549350738525, 0.1601131409406662, 0.1650838851928711, 0.18151068687438965, 0.16223827004432678, 0.15513423085212708, 0.18289867043495178, 0.1747286021709442, 0.16914239525794983, 0.13730904459953308, 0.1668630987405777, 0.170296311378479, 0.19254541397094727, 0.17963790893554688, 0.13823947310447693, 0.145034521818161, 0.1642032414674759, 0.19151413440704346, 0.17326335608959198, 0.1393221616744995, 0.17022249102592468, 0.17806905508041382, 0.15056827664375305, 0.156490296125412, 0.158961683511734, 0.1800379455089569, 0.16238190233707428, 0.1683199405670166, 0.16336697340011597, 0.13983534276485443, 0.15912383794784546, 0.16212795674800873, 0.1705794632434845, 0.16280505061149597, 0.1723448634147644, 0.15354925394058228, 0.16899386048316956, 0.1628284603357315, 0.1603328287601471, 0.14648757874965668, 0.16831287741661072, 0.1593039631843567, 0.17346413433551788, 0.15928810834884644, 0.15734830498695374, 0.1656167358160019, 0.16816481947898865, 0.15010283887386322, 0.15518580377101898, 0.17310774326324463, 0.15238694846630096, 0.1651388555765152, 0.16207942366600037, 0.1315310001373291, 0.14912894368171692, 0.15014415979385376, 0.1647641658782959, 0.17579269409179688, 0.17597942054271698, 0.17164283990859985, 0.1708516925573349, 0.16294053196907043, 0.1533602774143219, 0.15745660662651062, 0.1446734070777893, 0.15582714974880219, 0.15048816800117493, 0.1414380669593811, 0.14627666771411896, 0.16957594454288483, 0.15530143678188324, 0.15032507479190826, 0.16405025124549866, 0.17903587222099304, 0.1433020979166031, 0.1389344334602356, 0.14452050626277924, 0.14091043174266815, 0.13615091145038605, 0.15625689923763275, 0.1686333417892456, 0.1819528341293335, 0.1648985892534256, 0.17276373505592346, 0.15765349566936493, 0.15751482546329498, 0.13157597184181213, 0.16245898604393005, 0.15944334864616394, 0.16062690317630768, 0.17444053292274475, 0.15735188126564026, 0.14871732890605927, 0.15888860821723938, 0.13740521669387817, 0.16152817010879517, 0.1763642281293869, 0.14616096019744873, 0.15480898320674896, 0.16160999238491058, 0.14949853718280792, 0.17457881569862366, 0.16802626848220825, 0.13912715017795563, 0.15183216333389282, 0.13622885942459106, 0.15714290738105774, 0.14844098687171936, 0.14405719935894012, 0.18210087716579437, 0.14542698860168457, 0.15887904167175293, 0.15581119060516357, 0.15482385456562042, 0.16885453462600708, 0.13640403747558594, 0.1684655249118805, 0.17227604985237122, 0.17486688494682312, 0.14661000669002533, 0.15304505825042725, 0.146109938621521, 0.17087309062480927, 0.13562318682670593, 0.14693456888198853, 0.16767051815986633, 0.15963290631771088, 0.14197874069213867, 0.16163380444049835, 0.15618374943733215, 0.14173202216625214, 0.17257869243621826, 0.156894251704216, 0.13986647129058838, 0.13158607482910156, 0.15337440371513367, 0.15825563669204712, 0.13184300065040588, 0.15530118346214294, 0.14580313861370087, 0.14634965360164642, 0.14757326245307922, 0.14085103571414948, 0.15225817263126373, 0.15520654618740082, 0.15169532597064972, 0.14230692386627197, 0.17474986612796783, 0.16111811995506287, 0.17682895064353943, 0.16197018325328827, 0.13930393755435944, 0.1564975082874298, 0.13215786218643188, 0.14416414499282837, 0.18691585958003998, 0.1621258556842804, 0.16758489608764648, 0.1641799509525299, 0.13659369945526123, 0.16918596625328064, 0.1494435966014862, 0.17224250733852386, 0.16981709003448486, 0.16620118916034698, 0.16825953125953674, 0.19432438910007477, 0.15629246830940247, 0.16544179618358612, 0.15326175093650818, 0.1569470763206482, 0.16084598004817963, 0.14769774675369263, 0.14430060982704163, 0.15841223299503326, 0.16453933715820312, 0.1566108763217926, 0.15861259400844574, 0.14364862442016602, 0.14739668369293213, 0.16746973991394043, 0.16954883933067322, 0.1778157353401184, 0.15042465925216675, 0.1552782505750656, 0.153146430850029, 0.15274061262607574, 0.16219714283943176, 0.1648508608341217, 0.1553637534379959, 0.14689195156097412, 0.13999511301517487, 0.15881037712097168, 0.16331610083580017, 0.16896508634090424, 0.17008109390735626, 0.1501731425523758, 0.15985888242721558] [0.1526324599981308, 0.13128072023391724, 0.1727433204650879, 0.1600993573665619, 0.16116151213645935, 0.16015036404132843, 0.172317773103714, 0.17548903822898865, 0.14801061153411865, 0.1386961042881012, 0.13685372471809387, 0.15473730862140656, 0.1629830300807953, 0.15075558423995972, 0.14748220145702362, 0.16082146763801575, 0.14780454337596893, 0.1608911156654358, 0.1409069448709488, 0.14270931482315063, 0.15721504390239716, 0.14224526286125183, 0.17720258235931396, 0.1628245711326599, 0.1504291296005249, 0.15682244300842285, 0.13345426321029663, 0.16363760828971863, 0.15224042534828186, 0.1515267789363861, 0.18140709400177002, 0.15872983634471893, 0.15740428864955902, 0.15852424502372742, 0.14768928289413452, 0.16316139698028564, 0.16148903965950012, 0.15198582410812378, 0.1592688113451004, 0.13140806555747986, 0.16697993874549866, 0.15394288301467896, 0.15527325868606567, 0.15036174654960632, 0.16694584488868713, 0.16946321725845337, 0.14267697930335999, 0.1630057692527771, 0.16673439741134644, 0.15105819702148438, 0.14492537081241608, 0.1562574803829193, 0.1739625483751297, 0.15917164087295532, 0.1526481807231903, 0.15369614958763123, 0.14477255940437317, 0.1411098837852478, 0.15501224994659424, 0.16038253903388977, 0.15009090304374695, 0.13805420696735382, 0.16568660736083984, 0.16489538550376892, 0.17531555891036987, 0.17854666709899902, 0.14664259552955627, 0.14233684539794922, 0.160943403840065, 0.16614344716072083, 0.15495681762695312, 0.16174861788749695, 0.15219438076019287, 0.14932264387607574, 0.14354953169822693, 0.15841680765151978, 0.1566186398267746, 0.1466808319091797, 0.14902493357658386, 0.14670351147651672, 0.12839649617671967, 0.17517338693141937, 0.13931652903556824, 0.14397266507148743, 0.15640263259410858, 0.1675386279821396, 0.15077781677246094, 0.1535448431968689, 0.15919294953346252, 0.143513023853302, 0.14633923768997192, 0.1364658921957016, 0.13097354769706726, 0.16143566370010376, 0.142880380153656, 0.15419171750545502, 0.16118204593658447, 0.14958924055099487, 0.14992916584014893, 0.13072164356708527, 0.15429121255874634, 0.18933331966400146, 0.1544760763645172, 0.1576521247625351, 0.15261602401733398, 0.1350816935300827, 0.14624574780464172, 0.14123278856277466, 0.15959063172340393, 0.15611803531646729, 0.16843542456626892, 0.16050051152706146, 0.14211422204971313, 0.165291890501976, 0.17335794866085052, 0.1447867602109909, 0.13911627233028412, 0.12370532006025314, 0.15383203327655792, 0.13757023215293884, 0.17584984004497528, 0.15054036676883698, 0.1484510600566864, 0.18035802245140076, 0.17322593927383423, 0.13129860162734985, 0.1665155589580536, 0.1548769176006317, 0.15171076357364655, 0.16594752669334412, 0.1514652669429779, 0.1470833122730255, 0.1423250287771225, 0.16353097558021545, 0.1487407684326172, 0.1546831578016281, 0.14206835627555847, 0.14322876930236816, 0.14491073787212372, 0.1791170984506607, 0.14939364790916443, 0.13186614215373993, 0.14009618759155273, 0.1716705560684204, 0.15562863647937775, 0.14882808923721313, 0.141172394156456, 0.1425468623638153, 0.1476171314716339, 0.15915332734584808, 0.15383386611938477, 0.14611537754535675, 0.1443212628364563, 0.16538390517234802, 0.1482439786195755, 0.14678651094436646, 0.151850625872612, 0.15203532576560974, 0.13985365629196167, 0.172792449593544, 0.1539827287197113, 0.15759578347206116, 0.1611918956041336, 0.1499985307455063, 0.1302383542060852, 0.13614557683467865, 0.14203466475009918, 0.15090888738632202, 0.15207284688949585, 0.14308764040470123, 0.16464468836784363, 0.19561320543289185, 0.15470123291015625, 0.16547329723834991, 0.15038253366947174, 0.15047350525856018, 0.14467716217041016, 0.15681415796279907, 0.1523524820804596, 0.14747509360313416, 0.16399306058883667, 0.13523916900157928, 0.15535128116607666, 0.14975593984127045, 0.16605305671691895, 0.13486456871032715, 0.1536082923412323, 0.1257869005203247, 0.15645572543144226, 0.15754041075706482, 0.1682392954826355, 0.16145990788936615, 0.13033294677734375, 0.14632536470890045, 0.15699276328086853, 0.1537971943616867, 0.17445437610149384, 0.14772719144821167, 0.17021812498569489, 0.1513303518295288, 0.17600953578948975, 0.16840556263923645, 0.1642301380634308, 0.13564832508563995, 0.15415215492248535, 0.16997259855270386, 0.13967148959636688, 0.14743134379386902, 0.15946632623672485, 0.1451958417892456, 0.140313059091568, 0.13887359201908112, 0.1380981206893921, 0.14850200712680817, 0.18094748258590698, 0.1326548159122467, 0.16196075081825256, 0.16192185878753662, 0.14293727278709412, 0.16337350010871887, 0.16128279268741608, 0.16461938619613647, 0.16040754318237305, 0.1372101753950119, 0.14993485808372498, 0.13003841042518616, 0.14024442434310913, 0.15427157282829285, 0.15787562727928162, 0.14608804881572723, 0.1658822000026703, 0.160869762301445, 0.14505018293857574, 0.1630658358335495, 0.14516708254814148, 0.14537236094474792, 0.17154937982559204, 0.160666823387146, 0.14595869183540344, 0.17812387645244598, 0.1551210731267929, 0.1484123170375824, 0.13745826482772827, 0.1290765255689621, 0.1607557237148285, 0.16381382942199707, 0.1658410131931305, 0.13420331478118896, 0.16388148069381714, 0.14394688606262207, 0.15290966629981995, 0.15734849870204926, 0.13583603501319885, 0.16749146580696106, 0.13672614097595215, 0.14971330761909485, 0.14423419535160065, 0.15237189829349518, 0.13666215538978577, 0.16052943468093872, 0.14762842655181885, 0.16642840206623077, 0.14443206787109375, 0.15771207213401794, 0.15742456912994385, 0.14339759945869446, 0.1783551424741745, 0.13689440488815308, 0.14954161643981934, 0.16037753224372864, 0.1474265456199646, 0.14215120673179626, 0.14924487471580505, 0.1561674028635025, 0.16525131464004517, 0.13430675864219666, 0.15808415412902832, 0.14131826162338257, 0.143856018781662, 0.15858067572116852, 0.17747420072555542, 0.14729398488998413, 0.14158877730369568, 0.14068913459777832, 0.1406702697277069, 0.1355341076850891, 0.14140373468399048, 0.1809878945350647, 0.1420578956604004, 0.14653825759887695, 0.13812746107578278, 0.15906652808189392, 0.1460205614566803, 0.16007845103740692, 0.16044160723686218, 0.14768081903457642, 0.14229199290275574, 0.16286979615688324, 0.15080168843269348, 0.1722136288881302, 0.14820627868175507, 0.16649079322814941, 0.13739702105522156, 0.1577329784631729, 0.14771930873394012, 0.15072663128376007, 0.15760092437267303, 0.14409595727920532, 0.1602858006954193, 0.14199751615524292, 0.15846651792526245, 0.1684401035308838, 0.16179820895195007, 0.1497274935245514, 0.14920587837696075, 0.14207243919372559, 0.15008851885795593, 0.1424291431903839, 0.13878408074378967, 0.142167329788208, 0.1308925449848175, 0.1422024965286255, 0.14330163598060608, 0.16414524614810944, 0.15526001155376434, 0.14363855123519897, 0.16935741901397705, 0.14268922805786133, 0.14238590002059937, 0.1276552379131317, 0.13614997267723083, 0.14898191392421722, 0.17232534289360046, 0.16336527466773987, 0.14516659080982208, 0.14760352671146393, 0.14296019077301025, 0.13852828741073608, 0.15844203531742096, 0.16058500111103058, 0.1521589159965515, 0.13649210333824158, 0.15615898370742798, 0.14593705534934998, 0.1421736776828766, 0.14187878370285034, 0.1463741958141327, 0.13987761735916138, 0.1385931372642517, 0.17245009541511536, 0.1568101942539215, 0.14327934384346008, 0.14174045622348785, 0.14982709288597107, 0.14399737119674683, 0.13046693801879883, 0.13919243216514587, 0.16502031683921814, 0.1491435468196869, 0.17326560616493225, 0.1723441183567047, 0.15812957286834717, 0.13815924525260925, 0.1340947449207306, 0.1423126608133316, 0.12906323373317719, 0.1454637199640274, 0.16354170441627502, 0.14004956185817719, 0.14595913887023926, 0.1509225070476532, 0.13391989469528198, 0.1410381942987442, 0.1454470455646515, 0.13779231905937195, 0.14884142577648163, 0.13303586840629578, 0.15219071507453918, 0.14668059349060059, 0.17532514035701752, 0.13554252684116364, 0.1722247302532196, 0.13781079649925232, 0.14492689073085785, 0.1289372742176056, 0.15548843145370483, 0.139051154255867, 0.14530912041664124, 0.15601718425750732, 0.1412735879421234, 0.14679551124572754, 0.15378138422966003, 0.1618185043334961, 0.1245032548904419, 0.1426239311695099, 0.15412846207618713, 0.13380438089370728, 0.1617291271686554, 0.14893466234207153, 0.13857373595237732, 0.12743908166885376, 0.15653660893440247, 0.1471836417913437, 0.12842802703380585, 0.15630760788917542, 0.1342020332813263, 0.1541021764278412, 0.16311004757881165, 0.16249480843544006, 0.17021657526493073, 0.15222948789596558, 0.15527290105819702, 0.15801572799682617, 0.14902789890766144, 0.17015869915485382, 0.1542016714811325, 0.14743798971176147, 0.16362860798835754, 0.1435234248638153, 0.16135820746421814, 0.13793319463729858, 0.1520974338054657, 0.14225707948207855, 0.14684614539146423, 0.1480090171098709, 0.13705918192863464, 0.13153526186943054, 0.1513068675994873, 0.13828271627426147, 0.14743101596832275, 0.15305203199386597, 0.13967204093933105, 0.1486450433731079, 0.1504184454679489, 0.14743536710739136, 0.13584038615226746, 0.13174250721931458, 0.17313507199287415, 0.14991171658039093, 0.15406295657157898, 0.15446043014526367, 0.13926932215690613, 0.1456652283668518, 0.1515890210866928, 0.1514764130115509, 0.1338624209165573, 0.14764931797981262, 0.15029102563858032, 0.1270301342010498, 0.14232394099235535, 0.1463073194026947, 0.15976271033287048, 0.14593049883842468, 0.15599440038204193, 0.12716074287891388, 0.14452601969242096, 0.14897677302360535, 0.15059298276901245, 0.13003157079219818, 0.13476595282554626, 0.13861539959907532, 0.16585268080234528, 0.15730717778205872, 0.15003782510757446, 0.16196563839912415, 0.15188142657279968, 0.14470921456813812, 0.1483026146888733, 0.1512850522994995, 0.15637004375457764, 0.14441263675689697, 0.1415424644947052, 0.1584518700838089, 0.12371980398893356, 0.14517980813980103, 0.15209025144577026, 0.14237819612026215, 0.14334715902805328, 0.14426881074905396, 0.1457367241382599, 0.13606016337871552, 0.13310474157333374, 0.1417500376701355, 0.13281592726707458, 0.1507197618484497, 0.13082429766654968, 0.14714816212654114, 0.15042118728160858, 0.16539601981639862, 0.15688781440258026, 0.15112462639808655, 0.1403086632490158, 0.14147481322288513, 0.15413913130760193, 0.14080893993377686, 0.14800342917442322, 0.14376777410507202, 0.14143319427967072, 0.1417424976825714, 0.1600932478904724, 0.11958541721105576, 0.1400679349899292, 0.146688774228096, 0.1606535017490387, 0.15475967526435852, 0.14911817014217377, 0.13472303748130798, 0.16193950176239014, 0.13871584832668304, 0.16567584872245789, 0.15141233801841736, 0.15001346170902252, 0.1365416795015335, 0.16518676280975342, 0.14829328656196594, 0.1418546438217163, 0.12883782386779785, 0.15642490983009338, 0.12690937519073486, 0.15092186629772186, 0.15904895961284637, 0.1495938003063202, 0.12809348106384277, 0.14508023858070374, 0.14635078608989716, 0.15301355719566345, 0.14667680859565735, 0.14337611198425293, 0.14657887816429138, 0.15009954571723938, 0.13552653789520264, 0.13600799441337585, 0.1323576420545578, 0.14926961064338684, 0.12804366648197174, 0.14307087659835815, 0.13758453726768494, 0.13758698105812073, 0.16477277874946594, 0.1454089879989624, 0.14892488718032837, 0.15068617463111877, 0.14356370270252228, 0.1484864503145218, 0.14570267498493195, 0.12690028548240662, 0.1426374316215515, 0.13966047763824463, 0.13193562626838684, 0.1337887942790985, 0.16388526558876038, 0.16046515107154846, 0.1489838808774948, 0.1533474326133728, 0.12720945477485657, 0.14634960889816284, 0.14140935242176056, 0.13272738456726074, 0.14411824941635132, 0.15834280848503113, 0.1247989684343338, 0.14325375854969025, 0.13294796645641327, 0.14038679003715515, 0.14272955060005188, 0.15581680834293365, 0.1485447883605957, 0.14036644995212555, 0.14099162817001343, 0.14681273698806763, 0.133066326379776, 0.1353943645954132, 0.1614411473274231, 0.12668758630752563, 0.15488755702972412, 0.14119812846183777, 0.1358385980129242, 0.13608241081237793, 0.14226126670837402, 0.14187636971473694, 0.13579240441322327, 0.13027000427246094, 0.14848290383815765, 0.12678413093090057, 0.12740647792816162, 0.14914600551128387, 0.13659188151359558, 0.13638398051261902, 0.14988692104816437, 0.15301118791103363, 0.14021073281764984, 0.14995133876800537, 0.14588969945907593, 0.15329554677009583, 0.15281161665916443, 0.1368390917778015, 0.13386806845664978, 0.14045089483261108, 0.168931782245636, 0.16218498349189758, 0.13903312385082245, 0.12893345952033997, 0.17907756567001343, 0.14218664169311523, 0.12564897537231445, 0.13715678453445435, 0.16012176871299744, 0.1430494785308838, 0.15901094675064087, 0.13246464729309082, 0.13104096055030823, 0.13483059406280518, 0.1546345055103302, 0.12904755771160126, 0.13510294258594513, 0.13296973705291748, 0.13489484786987305, 0.16197478771209717, 0.13190367817878723, 0.15482386946678162, 0.14904513955116272, 0.11652793735265732, 0.13241466879844666, 0.1543436050415039, 0.13674870133399963, 0.1478070616722107, 0.15096962451934814, 0.12347589433193207, 0.13980749249458313, 0.1328733116388321, 0.14199793338775635, 0.13558274507522583, 0.1573636531829834, 0.13130411505699158, 0.14462044835090637, 0.16713404655456543, 0.15867391228675842, 0.1239795833826065, 0.14259910583496094, 0.1706368625164032, 0.14552319049835205, 0.143006831407547, 0.13809917867183685, 0.14312107861042023, 0.14828193187713623, 0.1265738159418106, 0.12761646509170532, 0.15517717599868774, 0.12644559144973755, 0.12570932507514954, 0.13265632092952728, 0.12996909022331238, 0.1451444923877716, 0.13016723096370697, 0.13060985505580902, 0.14206425845623016, 0.1323278546333313, 0.14430084824562073, 0.1338416188955307, 0.1395108997821808, 0.1421632468700409, 0.1419941782951355, 0.1254357397556305, 0.1430802047252655, 0.1319849193096161, 0.1427941918373108, 0.15332794189453125, 0.13694122433662415, 0.16438978910446167, 0.1543799340724945, 0.1453109085559845, 0.1377294361591339, 0.14618778228759766, 0.13682091236114502, 0.13876000046730042, 0.15210974216461182, 0.1373165398836136, 0.1460844874382019, 0.14032131433486938, 0.14419513940811157, 0.14394167065620422, 0.15863624215126038, 0.13773813843727112, 0.13464456796646118, 0.13816455006599426, 0.16433042287826538, 0.14005470275878906, 0.14141568541526794, 0.1410961002111435, 0.13550826907157898, 0.14341889321804047, 0.12487049400806427, 0.1428215205669403, 0.13160812854766846, 0.14246775209903717, 0.13838744163513184, 0.1326247602701187, 0.16741466522216797, 0.13702434301376343, 0.14696328341960907, 0.13601191341876984, 0.13658064603805542, 0.14250054955482483, 0.1315978318452835, 0.13453274965286255, 0.14139944314956665, 0.12821730971336365, 0.1325397789478302, 0.13499753177165985, 0.12099678814411163, 0.14993083477020264, 0.14707085490226746, 0.140890434384346, 0.14082060754299164, 0.12693846225738525, 0.13498523831367493, 0.11820020526647568, 0.14917975664138794, 0.14948217570781708, 0.1280624121427536, 0.12389639019966125, 0.14630064368247986, 0.1400858461856842, 0.13748785853385925, 0.13595446944236755, 0.1387578248977661, 0.1426468789577484, 0.143085315823555, 0.1517423391342163, 0.14985081553459167, 0.12015105038881302, 0.14282961189746857, 0.12522518634796143, 0.14813768863677979, 0.1411108821630478, 0.14313489198684692, 0.1497669517993927, 0.1260521560907364, 0.14018553495407104, 0.15302258729934692, 0.1316850632429123, 0.1305145025253296, 0.14362190663814545, 0.12785698473453522, 0.13130848109722137, 0.14489693939685822, 0.13366368412971497, 0.1465093195438385, 0.14770980179309845, 0.12114128470420837, 0.14480635523796082, 0.1540701538324356, 0.11793801188468933, 0.113267682492733, 0.1412343680858612, 0.13416855037212372, 0.12286775559186935, 0.1288607120513916, 0.14198829233646393, 0.13429348170757294, 0.128371924161911, 0.13166439533233643, 0.15488116443157196, 0.16242308914661407, 0.14921477437019348, 0.12767420709133148, 0.13573887944221497, 0.14198559522628784, 0.13461709022521973, 0.14541958272457123, 0.15981115400791168, 0.12721849977970123, 0.13676874339580536, 0.1274299919605255, 0.1363229900598526, 0.12328436225652695, 0.12485723197460175, 0.15450845658779144, 0.12736567854881287, 0.13233081996440887, 0.13984674215316772, 0.13663753867149353, 0.13485386967658997, 0.13961482048034668, 0.12452714145183563, 0.1345953643321991, 0.13821381330490112, 0.14731842279434204, 0.14523100852966309, 0.14597907662391663, 0.14747339487075806, 0.17660537362098694, 0.14679698646068573, 0.12701593339443207, 0.1473272144794464, 0.13653498888015747, 0.13188320398330688, 0.1270865947008133, 0.14761945605278015, 0.1685422658920288, 0.14916738867759705, 0.12056943029165268, 0.15056195855140686, 0.1365058422088623, 0.1288011372089386, 0.1390785425901413, 0.14914433658123016, 0.11368609964847565, 0.11597826331853867, 0.15386193990707397, 0.13421407341957092, 0.1407930999994278, 0.127198725938797, 0.1196821928024292, 0.14647126197814941, 0.13500763475894928, 0.13234305381774902, 0.14049765467643738, 0.15181270241737366, 0.14299258589744568, 0.14912885427474976, 0.1413324922323227, 0.14637210965156555, 0.12812264263629913, 0.1578371524810791, 0.16295702755451202, 0.1457410305738449, 0.15873688459396362, 0.18231093883514404, 0.13308489322662354, 0.14768511056900024, 0.15057852864265442, 0.15050268173217773, 0.13524597883224487, 0.14084124565124512, 0.1641070544719696, 0.14332543313503265, 0.13632455468177795, 0.15440106391906738, 0.13914015889167786, 0.14978668093681335, 0.15046249330043793, 0.1510380506515503, 0.12154556810855865, 0.15015825629234314, 0.14190486073493958, 0.13748356699943542, 0.13842712342739105, 0.1379161775112152, 0.1133265495300293, 0.12446742504835129, 0.14946426451206207, 0.13718301057815552, 0.15034987032413483, 0.13251207768917084, 0.1485261470079422, 0.1361394226551056, 0.13026052713394165, 0.14799991250038147, 0.14058682322502136, 0.13293245434761047, 0.12044657766819, 0.14809396862983704, 0.12378758192062378, 0.14358067512512207, 0.14174072444438934, 0.13302168250083923, 0.13859212398529053, 0.16114330291748047, 0.15292929112911224, 0.1362876296043396, 0.13676360249519348, 0.1441967785358429, 0.14397218823432922, 0.15676085650920868, 0.13396668434143066, 0.13671714067459106, 0.12697669863700867, 0.13921523094177246, 0.12497010082006454, 0.15227040648460388, 0.14395369589328766, 0.15516826510429382, 0.14642198383808136, 0.14400815963745117, 0.11390992999076843, 0.14172585308551788, 0.14949332177639008, 0.14198032021522522, 0.14036089181900024, 0.14245161414146423, 0.12389548122882843, 0.14022943377494812, 0.1471339464187622, 0.1256028711795807, 0.12901288270950317, 0.12325617671012878, 0.12083691358566284, 0.13153505325317383, 0.15354257822036743, 0.12959975004196167, 0.12134047597646713, 0.13795438408851624, 0.15678726136684418, 0.13015560805797577, 0.1608303338289261, 0.14410394430160522, 0.13942936062812805, 0.14627781510353088, 0.13877259194850922, 0.13093721866607666, 0.13112592697143555, 0.14128397405147552, 0.1544651985168457, 0.1517449915409088, 0.1382347047328949, 0.14695250988006592, 0.1353476643562317, 0.1519356667995453, 0.13422395288944244, 0.13592034578323364, 0.12115274369716644, 0.11274437606334686, 0.13816842436790466, 0.1522388756275177, 0.1373128890991211, 0.15220126509666443, 0.13911385834217072, 0.14756134152412415, 0.14727553725242615, 0.12742897868156433, 0.1354776918888092, 0.12707503139972687, 0.12693741917610168, 0.13213369250297546, 0.14063724875450134, 0.12795059382915497, 0.12052630633115768] [0.13191094994544983, 0.15989470481872559, 0.15236395597457886, 0.1295708864927292, 0.13758569955825806, 0.14132586121559143, 0.16822901368141174, 0.12742960453033447, 0.13664686679840088, 0.1463504135608673, 0.14271068572998047, 0.1361263394355774, 0.1277124583721161, 0.11700908839702606, 0.14030541479587555, 0.15430232882499695, 0.14876462519168854, 0.13606658577919006, 0.13765013217926025, 0.14284579455852509, 0.14506232738494873, 0.1279357522726059, 0.12698665261268616, 0.15131601691246033, 0.13407664000988007, 0.13264591991901398, 0.13523727655410767, 0.13385042548179626, 0.13611829280853271, 0.14619392156600952, 0.15440970659255981, 0.13357502222061157, 0.11436575651168823, 0.1139841079711914, 0.14149919152259827, 0.136487677693367, 0.13414476811885834, 0.13454857468605042, 0.1446133553981781, 0.12356163561344147, 0.1217818632721901, 0.14555993676185608, 0.12476719915866852, 0.13185366988182068, 0.1197018027305603, 0.12365604192018509, 0.13357335329055786, 0.1591024249792099, 0.14260371029376984, 0.1251281499862671, 0.14222931861877441, 0.14171558618545532, 0.14115817844867706, 0.13271208107471466, 0.15968739986419678, 0.13916926085948944, 0.15502110123634338, 0.12786714732646942, 0.12404600530862808, 0.1335640847682953, 0.1310710459947586, 0.13189464807510376, 0.14799484610557556, 0.16742488741874695, 0.12823395431041718, 0.1345263421535492, 0.12744930386543274, 0.12861597537994385, 0.15260069072246552, 0.1375264823436737, 0.13830451667308807, 0.15168315172195435, 0.13700136542320251, 0.14967137575149536, 0.14446969330310822, 0.13859346508979797, 0.11912120133638382, 0.12137740850448608, 0.13775119185447693, 0.13721120357513428, 0.13545994460582733, 0.12981823086738586, 0.14670024812221527, 0.15806466341018677, 0.1330280303955078, 0.13909582793712616, 0.12327161431312561, 0.1194257065653801, 0.13455729186534882, 0.14410284161567688, 0.14000900089740753, 0.12416700273752213, 0.13008256256580353, 0.12899042665958405, 0.12686508893966675, 0.1477092206478119, 0.12755247950553894, 0.13953809440135956, 0.13308091461658478, 0.13102251291275024, 0.12707072496414185, 0.1564493626356125, 0.136793315410614, 0.13038063049316406, 0.14409606158733368, 0.13415978848934174, 0.13448423147201538, 0.12892943620681763, 0.12108977884054184, 0.13614502549171448, 0.1335473656654358, 0.15112215280532837] [] [] []\n"
     ]
    }
   ],
   "source": [
    "# print(*loss_list)\n",
    "\n",
    "# path = save_dir + \"loss.txt\"\n",
    "# os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "# with open(path, \"w\") as f:\n",
    "#     for loss in loss_list:\n",
    "#         f.write(\" \".join(map(str, loss)) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# with open(\"loss.txt\", \"r\") as f:\n",
    "#     loss_list = [list(map(float, line.split())) for line in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Solarize_Light2', '_classic_test_patch', '_mpl-gallery', '_mpl-gallery-nogrid', 'bmh', 'classic', 'dark_background', 'fast', 'fivethirtyeight', 'ggplot', 'grayscale', 'seaborn-v0_8', 'seaborn-v0_8-bright', 'seaborn-v0_8-colorblind', 'seaborn-v0_8-dark', 'seaborn-v0_8-dark-palette', 'seaborn-v0_8-darkgrid', 'seaborn-v0_8-deep', 'seaborn-v0_8-muted', 'seaborn-v0_8-notebook', 'seaborn-v0_8-paper', 'seaborn-v0_8-pastel', 'seaborn-v0_8-poster', 'seaborn-v0_8-talk', 'seaborn-v0_8-ticks', 'seaborn-v0_8-white', 'seaborn-v0_8-whitegrid', 'tableau-colorblind10']\n"
     ]
    }
   ],
   "source": [
    "# print(plt.style.available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "记录的loss数量: 5740\n",
      "最后一个loss: 0.15112215280532837\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# plt.style.use(\"default\")\n",
    "\n",
    "# y = [e for l in loss_list for e in l]\n",
    "# print(f\"记录的loss数量: {len(y)}\")\n",
    "# print(f\"最后一个loss: {y[-1]}\")\n",
    "\n",
    "# plt.figure()\n",
    "# plt.plot(y)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lose_something(output_list, lose_num):\n",
    "    if lose_num == 0:\n",
    "        return output_list\n",
    "    \n",
    "    lose_index = torch.randperm(len(output_list))[:lose_num]\n",
    "    losed_output_list = []\n",
    "\n",
    "    for i in range(len(output_list)):\n",
    "\n",
    "        if i in lose_index:\n",
    "\n",
    "            losed_output_list.append(torch.zeros_like(output_list[i]))\n",
    "        else:\n",
    "\n",
    "            losed_output_list.append(output_list[i])\n",
    "    return losed_output_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from dataset.image_dataset import ImageDataset\n",
    "from util.split_data import split_vector\n",
    "\n",
    "conv_segment.to(device)\n",
    "fc_segment.to(device)\n",
    "model.to(device)\n",
    "encoder.to(device)\n",
    "decoder.to(device)\n",
    "\n",
    "conv_segment.eval()\n",
    "fc_segment.eval()\n",
    "model.eval()\n",
    "encoder.eval()\n",
    "decoder.eval()\n",
    "\n",
    "def evaluation(loader, loss_num):\n",
    "    original_correct = 0\n",
    "    merge_correct = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        loader_tqdm = tqdm(\n",
    "            loader,\n",
    "            desc=f\"Evaluating...\",\n",
    "            bar_format=\"{l_bar}{bar:20}{r_bar}\",\n",
    "        )\n",
    "        for images, labels in loader_tqdm:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # split image tensor(64, 3, 32, 32) -> [tensor(64, 3, 32, 8) * K]\n",
    "            images_list = []\n",
    "            for _1, _2, start, end in split_data_range:\n",
    "                images_list.append(images[:, :, :, start:end].clone())\n",
    "        \n",
    "            pad = (0, 3, 0, 0)\n",
    "            images_list[-1] = F.pad(images_list[-1], pad, \"constant\", value=0)\n",
    "\n",
    "            _, predicted = torch.max(model(images).data, 1)\n",
    "            original_correct += (predicted == labels).sum().item()\n",
    "\n",
    "            output = conv_segment(images)\n",
    "            output = output.view(output.size(0), -1)\n",
    "            output = fc_segment(output)\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            merge_correct += (predicted == labels).sum().item()\n",
    "\n",
    "            imageDataset_list = [\n",
    "                ImageDataset(images) for images in images_list + encoder(images_list)\n",
    "            ]\n",
    "            output_list = []\n",
    "            for i in range(N):\n",
    "                imageDataset = imageDataset_list[i]\n",
    "                output = conv_segment(imageDataset.images)\n",
    "                output_list.append(output)\n",
    "            losed_output_list = lose_something(output_list, loss_num)\n",
    "            decoded_output_list = decoder(losed_output_list)\n",
    "            output = torch.cat(decoded_output_list, dim=3)\n",
    "            output = output.view(output.size(0), -1)\n",
    "\n",
    "            _, predicted = torch.max(fc_segment(output).data, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    print(f\"样本总数: {total}\")\n",
    "    print(\n",
    "        f\"原始模型(model) -> 预测正确数: {original_correct}, 预测准确率: {(100 * original_correct / total):.2f}%\"\n",
    "    )\n",
    "    print(\n",
    "        f\"原始模型(conv+fc) -> 预测正确数: {merge_correct}, 预测准确率: {(100 * merge_correct / total):.2f}%\"\n",
    "    )\n",
    "    print(\n",
    "        f\"使用Encoder和Decoder -> 预测正确数: {correct}, 预测准确率: {(100 * correct / total):.2f}%\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_num: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating...: 100%|████████████████████| 391/391 [01:04<00:00,  6.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "样本总数: 50000\n",
      "原始模型(model) -> 预测正确数: 43124, 预测准确率: 86.25%\n",
      "原始模型(conv+fc) -> 预测正确数: 43124, 预测准确率: 86.25%\n",
      "使用Encoder和Decoder -> 预测正确数: 42754, 预测准确率: 85.51%\n",
      "loss_num: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating...: 100%|████████████████████| 391/391 [01:04<00:00,  6.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "样本总数: 50000\n",
      "原始模型(model) -> 预测正确数: 43124, 预测准确率: 86.25%\n",
      "原始模型(conv+fc) -> 预测正确数: 43124, 预测准确率: 86.25%\n",
      "使用Encoder和Decoder -> 预测正确数: 40041, 预测准确率: 80.08%\n",
      "loss_num: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating...: 100%|████████████████████| 391/391 [01:04<00:00,  6.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "样本总数: 50000\n",
      "原始模型(model) -> 预测正确数: 43124, 预测准确率: 86.25%\n",
      "原始模型(conv+fc) -> 预测正确数: 43124, 预测准确率: 86.25%\n",
      "使用Encoder和Decoder -> 预测正确数: 36959, 预测准确率: 73.92%\n",
      "loss_num: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating...: 100%|████████████████████| 391/391 [01:04<00:00,  6.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "样本总数: 50000\n",
      "原始模型(model) -> 预测正确数: 43124, 预测准确率: 86.25%\n",
      "原始模型(conv+fc) -> 预测正确数: 43124, 预测准确率: 86.25%\n",
      "使用Encoder和Decoder -> 预测正确数: 32682, 预测准确率: 65.36%\n",
      "loss_num: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating...: 100%|████████████████████| 391/391 [01:04<00:00,  6.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "样本总数: 50000\n",
      "原始模型(model) -> 预测正确数: 43124, 预测准确率: 86.25%\n",
      "原始模型(conv+fc) -> 预测正确数: 43124, 预测准确率: 86.25%\n",
      "使用Encoder和Decoder -> 预测正确数: 26940, 预测准确率: 53.88%\n",
      "loss_num: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating...: 100%|████████████████████| 391/391 [01:04<00:00,  6.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "样本总数: 50000\n",
      "原始模型(model) -> 预测正确数: 43124, 预测准确率: 86.25%\n",
      "原始模型(conv+fc) -> 预测正确数: 43124, 预测准确率: 86.25%\n",
      "使用Encoder和Decoder -> 预测正确数: 18217, 预测准确率: 36.43%\n",
      "loss_num: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating...: 100%|████████████████████| 391/391 [01:04<00:00,  6.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "样本总数: 50000\n",
      "原始模型(model) -> 预测正确数: 43124, 预测准确率: 86.25%\n",
      "原始模型(conv+fc) -> 预测正确数: 43124, 预测准确率: 86.25%\n",
      "使用Encoder和Decoder -> 预测正确数: 5000, 预测准确率: 10.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 训练集\n",
    "for i in range(N + 1):\n",
    "    print(f\"loss_num: {i}\")\n",
    "    evaluation(train_loader, i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_num: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating...: 100%|████████████████████| 157/157 [00:13<00:00, 11.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "样本总数: 10000\n",
      "原始模型(model) -> 预测正确数: 7432, 预测准确率: 74.32%\n",
      "原始模型(conv+fc) -> 预测正确数: 7432, 预测准确率: 74.32%\n",
      "使用Encoder和Decoder -> 预测正确数: 7439, 预测准确率: 74.39%\n",
      "loss_num: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating...: 100%|████████████████████| 157/157 [00:13<00:00, 11.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "样本总数: 10000\n",
      "原始模型(model) -> 预测正确数: 7432, 预测准确率: 74.32%\n",
      "原始模型(conv+fc) -> 预测正确数: 7432, 预测准确率: 74.32%\n",
      "使用Encoder和Decoder -> 预测正确数: 7129, 预测准确率: 71.29%\n",
      "loss_num: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating...: 100%|████████████████████| 157/157 [00:13<00:00, 11.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "样本总数: 10000\n",
      "原始模型(model) -> 预测正确数: 7432, 预测准确率: 74.32%\n",
      "原始模型(conv+fc) -> 预测正确数: 7432, 预测准确率: 74.32%\n",
      "使用Encoder和Decoder -> 预测正确数: 6631, 预测准确率: 66.31%\n",
      "loss_num: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating...: 100%|████████████████████| 157/157 [00:13<00:00, 11.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "样本总数: 10000\n",
      "原始模型(model) -> 预测正确数: 7432, 预测准确率: 74.32%\n",
      "原始模型(conv+fc) -> 预测正确数: 7432, 预测准确率: 74.32%\n",
      "使用Encoder和Decoder -> 预测正确数: 6055, 预测准确率: 60.55%\n",
      "loss_num: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating...: 100%|████████████████████| 157/157 [00:13<00:00, 11.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "样本总数: 10000\n",
      "原始模型(model) -> 预测正确数: 7432, 预测准确率: 74.32%\n",
      "原始模型(conv+fc) -> 预测正确数: 7432, 预测准确率: 74.32%\n",
      "使用Encoder和Decoder -> 预测正确数: 5064, 预测准确率: 50.64%\n",
      "loss_num: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating...: 100%|████████████████████| 157/157 [00:13<00:00, 11.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "样本总数: 10000\n",
      "原始模型(model) -> 预测正确数: 7432, 预测准确率: 74.32%\n",
      "原始模型(conv+fc) -> 预测正确数: 7432, 预测准确率: 74.32%\n",
      "使用Encoder和Decoder -> 预测正确数: 3385, 预测准确率: 33.85%\n",
      "loss_num: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating...: 100%|████████████████████| 157/157 [00:13<00:00, 11.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "样本总数: 10000\n",
      "原始模型(model) -> 预测正确数: 7432, 预测准确率: 74.32%\n",
      "原始模型(conv+fc) -> 预测正确数: 7432, 预测准确率: 74.32%\n",
      "使用Encoder和Decoder -> 预测正确数: 1000, 预测准确率: 10.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 测试集\n",
    "for i in range(N + 1):\n",
    "    print(f\"loss_num: {i}\")\n",
    "    evaluation(test_loader, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载模型\n",
    "# encoder = MLPEncoder(num_in=K, num_out=R, in_dim=split_data_shape)\n",
    "# decoder = MLPDecoder(num_in=N, num_out=K, in_dim=split_conv_output_shape)\n",
    "\n",
    "# encoder.load_state_dict(torch.load(encoder_path))\n",
    "# decoder.load_state_dict(torch.load(decoder_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练 base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!! 专门针对CIFAR10数据集的数据增强和预处理\n",
    "\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# 设置数据增强和预处理\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),  # 对图像进行随机裁剪，裁剪后图像大小为32x32，边缘填充4个像素\n",
    "    transforms.RandomHorizontalFlip(),    # 50%的概率水平翻转图像\n",
    "    transforms.RandomRotation(15),        # 随机旋转图像±15度\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1), # 随机调整亮度、对比度、饱和度和色调\n",
    "    transforms.ToTensor(),                # 将图片转换为Tensor，并归一化到[0,1]\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)), # 标准化处理，使用CIFAR-10数据集的均值和标准差\n",
    "])\n",
    "\n",
    "# 测试集通常不应用太多变换，通常只进行标准化处理\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "# 创建数据加载器\n",
    "base_model_trainloader = DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=2)\n",
    "base_model_testloader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=2)\n",
    "\n",
    "# 现在可以使用trainloader和testloader在你的模型中进行训练和测试了\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练函数\n",
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    tqdm_train_loader = tqdm(train_loader, desc=f\"Train Epoch {epoch}\")\n",
    "    for idx, (data, target) in enumerate(tqdm_train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "        tqdm_train_loader.set_postfix(loss=loss.item())\n",
    "    train_loss /= idx + 1\n",
    "    print(f\"Train set: Average loss: {train_loss:.4f}, Accuracy: {(100 * correct / len(train_loader.dataset)):.2f}%\")\n",
    "    print('-'*50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试函数\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        tqdm_test_loader = tqdm(test_loader, desc=\"Test\")\n",
    "        for idx, (data, target) in enumerate(tqdm_test_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += criterion(output, target).item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    test_loss /= idx + 1\n",
    "    print(f\"Test set: Average loss: {test_loss:.4f}, Accuracy: {(100 * correct / len(test_loader.dataset)):.2f}%\")\n",
    "    print('-'*50 + '\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_MODEL_NUM_EPOCHS = 2\n",
    "BASE_MODEL_LR = 1e-3\n",
    "# BASE_MODEL_MOMENTUM = 0.8\n",
    "BASE_MODEL_WEIGHT_DECAY = 1e-5\n",
    "# BASE_MODEL_CLIP_NORM = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 0: 100%|██████████| 391/391 [00:33<00:00, 11.58it/s, loss=0.726]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: Average loss: 0.5888, Accuracy: 79.11%\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test: 100%|██████████| 157/157 [00:08<00:00, 18.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Average loss: 0.7775, Accuracy: 73.44%\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 1: 100%|██████████| 391/391 [00:32<00:00, 12.01it/s, loss=0.564]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: Average loss: 0.5129, Accuracy: 81.72%\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test: 100%|██████████| 157/157 [00:08<00:00, 18.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Average loss: 0.7666, Accuracy: 74.32%\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=BASE_MODEL_LR, weight_decay=BASE_MODEL_WEIGHT_DECAY)\n",
    "# optimizer = optim.SGD(model.parameters(), lr=BASE_MODEL_LR, momentum=BASE_MODEL_MOMENTUM, weight_decay=BASE_MODEL_WEIGHT_DECAY)\n",
    "\n",
    "criterion = CrossEntropyLoss()\n",
    "\n",
    "model.to(device)\n",
    "for epoch in range(BASE_MODEL_NUM_EPOCHS):\n",
    "    train(model, device, base_model_trainloader, optimizer, epoch)\n",
    "    test(model, device, base_model_testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset:\n",
      "训练集-> 总量: 50000, 正确数量: 43124, 准确率: 86.248%\n",
      "Test dataset:\n",
      "测试集-> 总量: 10000, 正确数量: 7432, 准确率: 74.32%\n"
     ]
    }
   ],
   "source": [
    "# Evaluation Process\n",
    "# 测试循环\n",
    "model.eval()  # 设置模型为评估模式\n",
    "\n",
    "print(\"Train dataset:\")\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():  # 在评估过程中不计算梯度\n",
    "    for data, target in base_model_trainloader:\n",
    "        # 将数据移动到设备上\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        output = model(data)\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        total += target.size(0)\n",
    "        correct += (predicted == target).sum().item()\n",
    "\n",
    "print(f\"训练集-> 总量: {total}, 正确数量: {correct}, 准确率: {100 * correct / total}%\")\n",
    "\n",
    "print(\"Test dataset:\")\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():  # 在评估过程中不计算梯度\n",
    "    for data, target in base_model_testloader:\n",
    "        # 将数据移动到设备上\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        output = model(data)\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        total += target.size(0)\n",
    "        correct += (predicted == target).sum().item()\n",
    "\n",
    "print(f\"测试集-> 总量: {total}, 正确数量: {correct}, 准确率: {100 * correct / total}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存模型\n",
    "now = datetime.datetime.now()\n",
    "filepath = f\"base_model/{TASK_CONFIG['MODEL']}/{TASK_CONFIG['TASK']}/model.pth\"\n",
    "dirpath = os.path.dirname(filepath)\n",
    "if not os.path.exists(dirpath):\n",
    "    os.makedirs(dirpath)\n",
    "torch.save(model.state_dict(), filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取模型\n",
    "# model = LeNet5(input_dim=(1, 28, 28), num_classes=10)\n",
    "# model.load_state_dict(torch.load(filepath))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 0: 100%|██████████| 391/391 [00:44<00:00,  8.84it/s, loss=1.4] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: Average loss: 1.7057, Accuracy: 35.49%\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test: 100%|██████████| 157/157 [00:15<00:00, 10.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Average loss: 1.3033, Accuracy: 51.39%\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 1: 100%|██████████| 391/391 [00:34<00:00, 11.49it/s, loss=0.991]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: Average loss: 1.1765, Accuracy: 57.29%\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test: 100%|██████████| 157/157 [00:14<00:00, 10.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Average loss: 0.9887, Accuracy: 65.25%\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 2: 100%|██████████| 391/391 [00:35<00:00, 11.08it/s, loss=0.731]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: Average loss: 0.9172, Accuracy: 67.62%\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test: 100%|██████████| 157/157 [00:15<00:00, 10.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Average loss: 0.8411, Accuracy: 71.01%\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 3: 100%|██████████| 391/391 [00:34<00:00, 11.43it/s, loss=0.731]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: Average loss: 0.7712, Accuracy: 73.15%\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test: 100%|██████████| 157/157 [00:14<00:00, 10.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Average loss: 0.7356, Accuracy: 74.22%\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 4: 100%|██████████| 391/391 [00:34<00:00, 11.38it/s, loss=0.799]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: Average loss: 0.6701, Accuracy: 76.73%\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test: 100%|██████████| 157/157 [00:14<00:00, 10.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Average loss: 0.6462, Accuracy: 77.99%\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 5: 100%|██████████| 391/391 [00:34<00:00, 11.34it/s, loss=0.622]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: Average loss: 0.5851, Accuracy: 79.63%\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test: 100%|██████████| 157/157 [00:15<00:00, 10.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Average loss: 0.6260, Accuracy: 78.68%\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 6: 100%|██████████| 391/391 [00:35<00:00, 11.14it/s, loss=0.535]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: Average loss: 0.5329, Accuracy: 81.41%\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test: 100%|██████████| 157/157 [00:16<00:00,  9.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Average loss: 0.5974, Accuracy: 80.01%\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 7: 100%|██████████| 391/391 [00:37<00:00, 10.52it/s, loss=0.709]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: Average loss: 0.4814, Accuracy: 83.23%\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test: 100%|██████████| 157/157 [00:16<00:00,  9.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Average loss: 0.6000, Accuracy: 79.99%\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 8: 100%|██████████| 391/391 [00:34<00:00, 11.40it/s, loss=0.382]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: Average loss: 0.4353, Accuracy: 84.67%\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test: 100%|██████████| 157/157 [00:14<00:00, 10.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Average loss: 0.5882, Accuracy: 80.70%\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 9: 100%|██████████| 391/391 [00:34<00:00, 11.47it/s, loss=0.477]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: Average loss: 0.3984, Accuracy: 86.11%\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test: 100%|██████████| 157/157 [00:14<00:00, 10.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Average loss: 0.5887, Accuracy: 80.64%\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# CIFAR_10 is the model for cifar-10 from GPT\n",
    "\n",
    "# from torch.nn import CrossEntropyLoss\n",
    "# import torch.optim as optim\n",
    "# from base_model.CIFAR10_CNN import CIFAR10_CNN\n",
    "\n",
    "# test_model = CIFAR10_CNN().to(device)\n",
    "# optimizer = optim.Adam(test_model.parameters(), lr=0.001)\n",
    "# criterion = CrossEntropyLoss()\n",
    "\n",
    "# for epoch in range(10):\n",
    "#     train(test_model, device, base_model_trainloader, optimizer, epoch)\n",
    "#     test(test_model, device, base_model_testloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ming",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
